{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#solving-combinatorial-problems-using-reinforcement-learning","title":"Solving Combinatorial Problems using Reinforcement Learning","text":"<p>Combinatorial optimization problems are frequently characterized by being intractable for classical algorithms or prone to suboptimal solutions. Such problems naturally arise in logistics (e.g., vehicle routing), scheduling (e.g., job shop), network design (e.g., flow), finance (e.g., portfolio selection), and beyond. Even minor improvements in these domains can yield substantial benefits in cost, efficiency, or strategy. However, designing heuristics and metaheuristics to tackle their complexity is time-consuming.</p> <p>Reinforcement Learning (RL) has excelled at sequential decision-making tasks in fields ranging from autonomous driving and industrial control to robotics, protein folding, theorem proving, and multiagent games such as chess and go, where it has achieved superhuman performance.</p> <p> </p> <p>In this exemplar, we will focus on learning to use Reinforcement Learning for solving sequential combinatorial problems, where an optimal strategy involves taking specific actions in a sequence while also responding to a probabilistic setting (environment). Notably, Reinforcement Learning is able to learn the state and action space, so it is able to effectively search these spaces for optimal solutions as opposed to exhaustive searches in classical algorithms, without any heuristics that require expert knowledge to correctly derive.</p> <p>We will start by implementing a foundational algorithm, Tabular Q Learning, then learn how to apply it in a pre-supplied environment, involving the famous Monty Hall problem, where we will also explore hyperparameter tuning and visualisation of training. After this, we will learn how you can apply RL to any problem space of interest by creating your own environment, where we will walk through an example implementing an environment from scratch for the seminal News Vendor problem from inventory management.</p> <p>This exemplar was developed at Imperial College London by Omar Adalat in collaboration with Dr. Diego Alonso Alvarez from Research Software Engineering and Dr. Jes\u00fas Urtasun Elizari from Research Computing &amp; Data Science at the Early Career Researcher Institute.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes \ud83c\udf93","text":"<p>After completing this exemplar, students will:</p> <ul> <li>Explain the core principles of Reinforcement Learning, and be able to identify when and where it is applicable to a problem space, with a particular focus on combinatorial problems for this project.</li> <li>Develop an implementation of a foundational algorithm, Tabular Q Learning, starting from basic principles and concepts.</li> <li>Gain the ability to perform experimental validation of the trained Reinforcement Learning algorithm, visualising the learning over training episodes.</li> <li>Design hyperparameter tuning configurations that can automatically be applied to retrieve the optimal set of hyperparameters.</li> <li>Generalise to non-supplied environments by learning how to create your own Reinforcement Learning environment, allowing you to apply Reinforcement Learning to any problem space that you are interested in.</li> </ul>"},{"location":"#target-audience","title":"Target Audience \ud83c\udfaf","text":"<p>This exemplar is broadly applicable to anyone interested in solving sequential decision problems, which comes up ubiquitously across many domains and industries (e.g. protein synthesis, self-driving cars, planning &amp; scheduling, and strategic games). Specifically, although we focus on sequential combinatorial problems which are a specific flavour of sequential decision problems, the underlying concepts are the same between both.</p> <p>Our exemplar suitable for students, researchers and engineers alike, and academic prerequisite knowledge is not assumed, aside from some confidence in using Python.</p>"},{"location":"#prerequisites","title":"Prerequisites \u2705","text":""},{"location":"#academic","title":"Academic \ud83d\udcda","text":"<ul> <li>Basic familiarity with Python &amp; basic programming skills is required to solve the exercises notebooks</li> <li>Some math background in the basics of set theory and probability theory is helpful but not required</li> </ul>"},{"location":"#system","title":"System \ud83d\udcbb","text":"<ul> <li>Astral's uv Python package and project manager</li> <li>An integrated development environment (IDE) for developing and running Python, Visual Studio Code (VS Code) with Python &amp; Jupyter Notebook extensions is the easiest to set up and use. VS Code should automatically prompt you to install the required extensions that you need, but you can refer to here for the Python extension and the Jupyter extension is available for notebook support</li> <li>10 GB of disk space</li> </ul>"},{"location":"#getting-started","title":"Getting Started \ud83d\ude80","text":"<ol> <li>Start by cloning the repository, either using the GitHub interface or Git directly (<code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-Solving-Combinatorial-Problems-using-Reinforcement-Learning</code>).</li> <li>Install Astral's <code>uv</code> if not already installed from the following URL: https://docs.astral.sh/uv/</li> <li>Run the command <code>uv sync</code> in the cloned repository directory. This will install the correct version of Python (scoped to the directory under <code>uv</code>) and gather all dependencies needed.</li> <li>Create a virtual environment under which the Jupyter Notebooks will run under, which will be scoped to the project directory. Simply run <code>uv venv --python 3.12</code>. When running any notebook, use the virtual environment created for Python 3.12 in the current directory's path, VS Code will give you a list selection of virtual environments to run under (you can also switch this in the top right of a notebook as of the time of writing).</li> <li>Ensure that your notebook's working directory is set to the root of the project directory (the top level folder that lists <code>notebooks</code>, <code>README.md</code>, <code>src</code>, etc). If you are using VS Code, this is already specified by <code>.vscode/settings.json</code> of this repository. If you are using a different IDE, be sure to specify this equivalently, it will be evident that this is working if notebooks are able to properly import all local imports.</li> <li>Navigate to the five notebooks in the directory <code>/notebooks/</code> and complete them in order, running the exercises which will be checked against automated tests and checking the solutions if at any time you are stuck!</li> </ol>"},{"location":"#disciplinary-background","title":"Disciplinary Background \ud83d\udd2c","text":"<p>Reinforcement Learning is a powerful learning paradigm in Artificial Intelligence &amp; Computer Science. While Deep Learning and general Machine Learning are very interesting, often the focus is on making a single isolated decision as in the tasks of classification or regression. Reinforcement Learning, which at the state-of-the-art level also uses Deep Learning for effective learning, is important to learn and master for solving more complex tasks that involve sequential decisions.</p> <p>Specifically, as it solves sequential decision problems, it is incredibly useful in an interdisciplinary manner for various problems that arise such as the aforementioned: scheduling, protein synthesis, finance, autonomous vehicles and beyond.</p>"},{"location":"#software-tools","title":"Software Tools \ud83d\udee0\ufe0f","text":"<ul> <li>Python, with version and dependencies managed by Astral's uv</li> <li>Weights &amp; Biases, a web platform for experiment tracking best practices and hyperparameter tuning</li> </ul> <p>A selection of key Python libraries/packages used are listed below: - Gymnasium, allowing to define custom RL environments which conform to a standard interface - Pygame, for visualisation of the environments - Matplotlib, for visualisation of training results by plotting charts and diagrams - Jupyter Notebooks, for literate programming &amp; interactive content</p>"},{"location":"#project-structure","title":"Project Structure \ud83d\uddc2\ufe0f","text":"<p>Overview of code organisation and structure.</p> <pre><code>.\n\u251c\u2500\u2500 notebooks\n\u2502 \u251c\u2500\u2500 1-Intro-to-RL.ipynb\n\u2502 \u251c\u2500\u2500 2-Tabular-Q-Learning.ipynb\n\u2502 \u251c\u2500\u2500 3-Experiments.ipynb\n\u2502 \u251c\u2500\u2500 4-Custom-Envs-News-Vendor.ipynb\n\u2502 \u251c\u2500\u2500 5-Conclusion.ipynb\n\u251c\u2500\u2500 src\n\u2502 \u251c\u2500\u2500 environments\n\u2502 \u251c\u2500\u2500\u2500\u2500\u2500 monty_hall\n\u2502       \u2502          \u2514\u2500\u2500 env.py\n\u2502       \u2502          \u2514\u2500\u2500 state.py\n\u2502       \u2502          \u2514\u2500\u2500 renderer.py\n\u2502       \u2502          \u2514\u2500\u2500 discrete_wrapper.py\n\u2502 \u251c\u2500\u2500\u2500\u2500\u2500 news_vendor\n\u2502       \u2502          \u2514\u2500\u2500 env.py\n\u2502       \u2502          \u2514\u2500\u2500 state.py\n\u2502       \u2502          \u2514\u2500\u2500 renderer.py\n\u2502       \u2502          \u2514\u2500\u2500 discrete_wrapper.py\n\u2502 \u251c\u2500\u2500 rl\n\u2502   \u2502   \u2514\u2500\u2500 common.py\n\u2502   \u2502   \u2514\u2500\u2500 tabular_q_learning.py\n\u251c\u2500\u2500 docs\n\u2514\u2500\u2500 test\n</code></pre> <p>Code is organised into logical components:</p> <ul> <li><code>notebooks</code> for tutorials and exercises<ul> <li><code>solutions</code> contains full solutions to all exercises, implementing all incomplete functions</li> <li><code>extras</code> contains notebooks that allow you to interactively visualise the Monty Hall and News Vendor environments</li> </ul> </li> <li><code>src</code> for core code<ul> <li><code>monty_hall</code> provides the full implementation of the Monty Hall Gymnasium environment. This is something you are expected to import in for Notebook 3. However, later on you can explore this directory in terms of how everything is implemented, for example the discrete state space wrapper, action masking, and visualisation. It may be useful as a reference for any environments you create in the future!</li> <li><code>news_vendor</code> is a full reference/target implementation for Notebook 4.</li> <li><code>rl</code> is a reference implementation for Notebook 2, particularly focused on Tabular Q Learning.</li> </ul> </li> <li><code>docs</code> for documentation</li> <li><code>test</code> for testing scripts</li> </ul>"},{"location":"#best-practice-notes","title":"Best Practice Notes \ud83d\udcdd","text":"<ul> <li>Package (dependency) management and Python version management is provided by <code>uv</code>, which allows a perfectly replicable development environment</li> <li>Reference code is entirely documented and commented using Google's Style of Python Docstrings</li> <li>Experiments are stored and tracked using Weights &amp; Biases, which allows long-term access to results of experiments, accompanied by all necessary information to replicate such experiments such as hyperparameters</li> </ul>"},{"location":"#estimated-time","title":"Estimated Time \u23f3","text":"Task Time Notebook 1. Intro to RL 3 hours Notebook 2. Tabular Q Learning 6 hours Notebook 3. Experiments 2 hours Notebook 4. Custom environment: News Vendor 4 hours Notebook 5. Conclusion 3 hours <p>Total time: 18 hours</p>"},{"location":"#additional-resources","title":"Additional Resources \ud83d\udd17","text":"<ul> <li>For building your Reinforcement Learning knowledge:<ul> <li>Mastering Reinforcement Learning, which is a book accompanied by videos, providing an excellent overview of the various Reinforcement Learning methods out there</li> <li>Reinforcement Learning: An Introduction, a seminal book with its latest edition published in 2018, by Richard S. Sutton and Andrew G. Barto. This book is considered foundational, and both authors heavily contributed to Reinforcement Learning research and helped start the field. However, this book is more on the theoretical side.</li> <li>Spinning Up in Deep RL by OpenAI, which provides a great overview of the state-of-the-art methods (e.g. PPO and actor-critic methods), particularly with deep reinforcement learning.<ul> <li>If you are not familiar with Deep Learning, consider looking at:<ul> <li>Dive into Deep Learning, free online book, with code accompanying each section</li> <li>fast.ai courses, covering advanced deep learning methods from the foundations accompanied by practical implementations</li> </ul> </li> </ul> </li> </ul> </li> <li>Additional combinatorial environments are available at:<ul> <li>Jumanji</li> <li>OR-gym, OR stands for Operations Research </li> </ul> </li> <li>Specifically for attaining better performance in combinatorial RL, you may want to investigate:<ul> <li>More advanced exploration methods, other than greedy-epsilon, starting with Boltzmann</li> <li>Pointer Networks, used by some methods such as AlphaStar</li> <li>Stochastic Q Learning for handling large action spaces</li> <li>Abstraction methods for lowering the complexity of the state and action space</li> </ul> </li> </ul>"},{"location":"#licence","title":"Licence \ud83d\udcc4","text":"<p>This project is licensed under the BSD-3-Clause license.</p>"},{"location":"1-Intro-to-RL/","title":"1. Introduction to Reinforcement Learning","text":"In\u00a0[1]: Copied! <pre>from utils.grader import make_tester\nfrom notebooks.internal.exercises_1_intro_to_rl import square_cases\n</pre> from utils.grader import make_tester from notebooks.internal.exercises_1_intro_to_rl import square_cases <p>When you multiply those constraints across an entire weekly timetable, the number of feasible crew pairings skyrockets\u2014well beyond what could ever be enumerated explicitly \u2014 so airlines rely on combinatorial optimization techniques (e.g. column generation, branch\u2011and\u2011price, heuristics) to find near\u2011optimal schedules. Note that near optimal is sufficient in many domains, always striving for optimality can require a much more exhaustive search.</p> <p> </p> <p>Reinforcement Learning is a sampling-based approach that learns in an online fashion, by actively taking actions in an environment, learning from experience. An agent (which in reality is any Reinforcement Learning algorithm) is said to be exploring and acting in the environment.</p> <p>Specifically, Reinforcement Learning involves the key following ingredients:</p> <ul> <li>states: also referred to as the observation/state space. It tells you what state the system is in, for example in the airline scenario, it would tell you where all the crew are currently located at, what their status is, and what the status of any unassigned/assigned flights are in. We start with an initial state of the state space, commonly referred to as $s_0$.</li> <li>actions: an agent can take a single action from the action space at each time step. Upon taking an action, such as assigning a flight crew to a particular flight, the environment advances one step, into a new resultant state, where the agent can take a subsequent action.</li> <li>reward: a reward is received upon taking an action in a state, that tells how useful the action was. In some environments, the reward can only be provided at the end of the task, this is termed as sparse reward. Learning is more efficient when the agent is provided a more dense reward, at each time step (following each action), but this is not ultimately required.</li> </ul> <p>The state space, action space, and reward function, is provided by a given environment. Any combinatorial problem space you can think of can be represented as a reinforcement learning environment. At each time step, the agent starts in a particular state, takes a desired action, and ends up in a new state for the next time step where this process repeats until a final time step.</p> <p>Crucial idea) By taking actions, the agent receives a reward, and can learn how effective their action trajectory was. Reinforcement Learning learns from experience, taking a combination of random (exploring) actions and optimal actions. Unlike classical Machine Learning, you do not need to provide any data set, only an environment that the agent can act and explore in.</p> <p>We will be considering episodic RL, that is a form of Reinforcement Learning with a finite number of time steps that is then terminated and restarts a new episode. This helps because the agent will also be reset into an initial state often enough to explore different strategies effectively.</p> <p>Specifically, the agent is trying to learn an optimal policy ($\\pi$), a policy simply tells the agent which action $a$ (from the action space) to take in any given state $s$ (from the entire state space). An optimal policy would be produce the highest amount of possible cumulative reward (cumulatively collated from each time step of the environment).</p> In\u00a0[2]: Copied! <pre># Implement and complete this!\ndef square(x):\n    return x\n</pre> # Implement and complete this! def square(x):     return x In\u00a0[3]: Copied! <pre>tester_widget = make_tester(square, square_cases)\ndisplay(tester_widget)\n</pre> tester_widget = make_tester(square, square_cases) display(tester_widget) <pre>VBox(children=(HBox(children=(Button(button_style='success', description='Run Tests', icon='check', style=Butt\u2026</pre>"},{"location":"1-Intro-to-RL/#1-introduction-to-reinforcement-learning","title":"1. Introduction to Reinforcement Learning\u00b6","text":""},{"location":"1-Intro-to-RL/#background-sequential-combinatorial-problems","title":"Background \u2014 Sequential Combinatorial Problems\u00b6","text":""},{"location":"1-Intro-to-RL/#sequential-decision-problems","title":"Sequential Decision Problems\u00b6","text":"<p>Solving sequential decision problems are problems where actions must be taken successively. This in contrast to many other Machine Learning domains where only a single decision is needed: for example classification (e.g. predicting whether an image belongs to a certain class, like whether it is a cat picture) or regression tasks (e.g. predicting the valuation of a house).</p>"},{"location":"1-Intro-to-RL/#combinatorial-problems","title":"Combinatorial Problems\u00b6","text":"<p>Specifically, we are focused on a specific type of sequential decision problems known as combinatorial problems. Combinatorial problems are questions about how discrete things can be arranged, selected, combined, or optimized. By discrete, we simply mean objects that are distinct and countable. Combinatorial problems suffer from exponential blow-up or combinatorial explosion, because choices multiply rapidly, quickly intensifying the amount of compute power needed. Here is an example of a combinatorial problem:</p> <p>Imagine a pizzeria that offers 8 distinct toppings (say: pepperoni, mushrooms, onions, olives, bell peppers, chicken, spinach, and pineapple). Each customer can choose any subset of these toppings \u2014 including the possibility of \"no toppings\" or \"all toppings.\"</p> <p>How many distinct types of pizzas can be made? In this case, it would be a staggering $2^8 = 256$ (which, as a bit of interesting background knowledge, more formally corresponds to something called the \"power set\").</p> <p>While this was just a fun example, in many other application domains, considering all combinations becomes important. Consider the following scenario:</p> <p>Every day an airline must assign thousands of pilots and flight attendants to thousands of flight legs. Each crew member has legal limits on duty hours, mandatory rest, base cities, training qualifications (e.g., aircraft type), and seniority\u2011based bidding preferences.</p>"},{"location":"1-Intro-to-RL/#realistic-domains","title":"Realistic Domains\u00b6","text":"<p>Combinatorial optimization problems are frequently characterized by being intractable for classical algorithms or prone to suboptimal solutions. However, combinatorial problems naturally arise in logistics (e.g., vehicle routing), scheduling (e.g., job shop), network design (e.g., flow), finance (e.g., portfolio selection), and beyond. Even minor improvements in these domains can yield substantial benefits in cost, efficiency, or strategy. However, designing heuristics and metaheuristics to tackle their complexity is time-consuming.</p>"},{"location":"1-Intro-to-RL/#heuristics","title":"Heuristics\u00b6","text":"<p>When solving combinatorial problems, there is often a reliance on heuristics (as well as something called \"metaheuristics\" which we can ignore for now). A heuristic is a rule\u2011of\u2011thumb (algorithm or decision policy) that helps speed up the search algorithm for a solution. Under the right conditions, a heuristic can still deliver an optimal solution, however this does not necessarily have to be the case. For the airline example, we can try a heuristic algorithm along the following lines:</p> <p>For every crew, keep a running \u201ccursor\u201d that points to the last flight they can still catch given legal connection times. Assign the earliest uncovered flight to the crew whose cursor makes it feasible and whose home base matches (ties broken by least additional duty time). This is a decent heuristic, but not completely optimal for a few reasons, including teh fact that it might leave some larger gaps which increase hotel costs, and also does not consider seniority bidding preferences (which may have to be considered as a second processing step involving swapping flights where possible).</p> <p>The underlying idea of heuristics is we want to avoid exhaustively searching the entire search space (involving all potential combinations) for the optimal solution. Rather, we want to explore a promising part of the search space. This promising part may contain the optimal solution, although, it may simply include a near-optimal solution.</p>"},{"location":"1-Intro-to-RL/#why-should-we-use-reinforcement-learning","title":"Why should we use Reinforcement Learning?\u00b6","text":"<p>Crafting reliable heuristics and metaheuristics is a very intensive task that requires a lot of domain expertise.</p> <p>On the other hand, Reinforcement Learning has excelled at sequential decision-making tasks in fields ranging from autonomous driving and industrial control to robotics, protein folding, theorem proving, and multiagent games such as Chess and Go, where it has achieved superhuman performance.</p> <p>Reinforcement Learning learns from experience, explicitly considering the trade-off between exploring (seeking out new knowledge or areas of the search space) and exploiting (doing what it knows to be the most optimal selection at a given time point).</p> <p>Therefore, implicitly, Reinforcement Learning can be stated to automatically learn heuristics and domain properties, without requiring any hand-coded knowledge.</p>"},{"location":"1-Intro-to-RL/#what-is-reinforcement-learning-exactly-a-high-level-overview","title":"What is Reinforcement Learning exactly? (A high-level overview)\u00b6","text":""},{"location":"1-Intro-to-RL/#course-structure","title":"Course Structure\u00b6","text":"<p>You have now almost completed notebook 1, well done! Following this, notebook 2 will cover the implementation of a foundational, and quite intuitive algorithm of Tabular Q Learning. Notebook 3 will teach you how to perform hyperparameter tuning (setting the best parameters for your RL algorithm to learn in) and visualise the training results, where you will be supplied with the Monty Hall environment. Finally, Notebook 4 will conclude with teaching you on how exactly you can define your own environment using Gymnasium, allowing you to apply RL to any problem space you can think of.</p>"},{"location":"1-Intro-to-RL/#solving-exercises-in-notebooks","title":"Solving Exercises in Notebooks\u00b6","text":"<p>Note: for the complete exemplar experience, you should work directly within the notebooks instead of viewing the exemplar through the website as interactive elements (e.g. exercises) are not functional on the website.</p> <p>These notebooks are defined with exercises in mind, involving little Python functions or potentially little bits of functions for you to complete. You will then receive some instant feedback using the \"run tests\" button on whether or not your implementation was correct.</p> <p>While reference code is provided in the repository, if you are able to, it is best to solve as many exercises without looking at any reference code or solutions. This will strengthen your understanding of how everything works by applying the knowledge you have gained.</p> <p>For example, try complete the following function, which requires you to simply write a simple mathematical squaring function, multiplying a number by itself and returning the result.</p>"},{"location":"1-Intro-to-RL/#glossary","title":"Glossary\u00b6","text":"<p>This glossary provides a quick reference of some of key concepts introduced within this notebook, which you may find helpful to refer to.</p> <ul> <li>action: an agent can take a single action from the action space at each time step. Upon taking an action, the environment advances one step, into a new resultant state capturing the effects of the last action, where the agent can now take a subsequent action.</li> <li>episode: in episodic RL, the agent can take a finite number of actions based on the number of time steps defined per episode, before the agent is reset to a new episode with a new initial state, taking its previous learning experiences with it into the new episode.</li> <li>environment: an agent can only learn with a given state space, action space, and reward function, and these are supplied by a defined environment.</li> <li>hyperparameter: a setting you choose before training that shapes how learning happens (e.g., learning rate, discount factor, exploration rate, network size). Hyperparameters aren\u2019t learned by the agent; you pick and tune them to balance speed, stability, and final performance.</li> <li>initial state: the agent starts in some initial state $s_0$ from the state-space of the environment. The initial state is a default configuration where no actions have been taken yet.</li> <li>reward: a reward is received upon taking an action in a state, that tells how useful the action was. In some environments, the reward can only be provided at the end of the task, this is termed as sparse reward. Learning is more efficient when the agent is provided a more dense reward, at each time step (following each action), but this is not ultimately required. The reward is governed by the reward function defined per environment.</li> <li>policy: governs which action $a$ from the action space that the agent will take in any given state $s$ from the state/observation space. An optimal policy is one which always takes the most optimal action (which produces the highest rewards) from any given state.</li> <li>state: also called the observation space. It tells you what state the system is in, capturing all active facts of the environment. We start with an initial state of the state space, commonly referred to as $s_0$.</li> <li>time step: a single time step advances after taking a single (atomic) action. Also see episode or episodic RL.</li> <li>tabular Q-learning: a simple, classic RL method that keeps a table of values (Q-values) for each state\u2013action pair in a table. As the agent interacts, it updates the table based on observed rewards and then chooses actions with the highest values, gradually improving its behaviour without needing a model of the environment.</li> </ul>"},{"location":"2-Tabular-Q-Learning/","title":"2. Tabular Q Learning","text":"In\u00a0[1]: Copied! <pre>from utils.grader import *\nfrom notebooks.internal.exercises_2_tabular_q_learning import *\n\nimport numpy as np\n</pre> from utils.grader import * from notebooks.internal.exercises_2_tabular_q_learning import *  import numpy as np <p>When the state and action spaces are discrete and not too large, $Q$ can be stored explicitly as a table, with one scalar entry for each $(s, a)$ pair. In larger problems, we would have to approximate $Q$ with a parameterised function (e.g. a neural network).</p> <p>That is all Tabular Q\u2011Learning is: training a table that tells you how valuable every possible state\u2013action pair is.</p> In\u00a0[2]: Copied! <pre>def epsilon_greedy(\n    values: np.ndarray,\n    epsilon: float,\n    mask: np.ndarray | None = None,\n    seed: int | None = None,\n) -&gt; int:\n    \"\"\"Return an action index drawn from an \u03b5-greedy policy.\n\n    Args:\n        q_values (1d NumPy array): current estimated q-values.\n        epsilon (float scalar): Exploration rate between 0 and 1\n        mask (np.ndarray | None, optional):\n            Binary vector marking **legal** actions (``1``\u00a0\u2192 legal, ``0``\u00a0\u2192 illegal).\n            If *None* every action is assumed to be legal.\n        seed (int | None, optional):\n            If provided, a fresh RNG initialised with this seed is used, making\n            the call fully deterministic.\n    Returns:\n            int: Index of the chosen action.\n    \"\"\"\n    rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()\n\n    # Determine the set of legal actions --------------------------------------\n    legal = np.arange(values.size) if mask is None else np.flatnonzero(mask)\n    if legal.size == 0:\n        raise ValueError(\"No legal actions available\")\n\n    # Exploration -------------------------------------------------------------\n    # TODO: Implement this!\n    raise NotImplementedError\n\n    # Exploitation ------------------------------------------------------------\n    best_idx_within_legal = legal[np.argmax(values[legal])]\n    # TODO: Implement this!\n    raise NotImplementedError\n\nmake_tester(epsilon_greedy, epsilon_greedy_cases)\n</pre> def epsilon_greedy(     values: np.ndarray,     epsilon: float,     mask: np.ndarray | None = None,     seed: int | None = None, ) -&gt; int:     \"\"\"Return an action index drawn from an \u03b5-greedy policy.      Args:         q_values (1d NumPy array): current estimated q-values.         epsilon (float scalar): Exploration rate between 0 and 1         mask (np.ndarray | None, optional):             Binary vector marking **legal** actions (``1``\u00a0\u2192 legal, ``0``\u00a0\u2192 illegal).             If *None* every action is assumed to be legal.         seed (int | None, optional):             If provided, a fresh RNG initialised with this seed is used, making             the call fully deterministic.     Returns:             int: Index of the chosen action.     \"\"\"     rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()      # Determine the set of legal actions --------------------------------------     legal = np.arange(values.size) if mask is None else np.flatnonzero(mask)     if legal.size == 0:         raise ValueError(\"No legal actions available\")      # Exploration -------------------------------------------------------------     # TODO: Implement this!     raise NotImplementedError      # Exploitation ------------------------------------------------------------     best_idx_within_legal = legal[np.argmax(values[legal])]     # TODO: Implement this!     raise NotImplementedError  make_tester(epsilon_greedy, epsilon_greedy_cases) Out[2]: <pre>VBox(children=(HBox(children=(Button(button_style='success', description='Run Tests', icon='check', style=Butt\u2026</pre> In\u00a0[3]: Copied! <pre>def td_update(\n    q_table: np.ndarray,\n    state_idx: int,\n    action_idx: int,\n    reward: float,\n    next_state_idx: int,\n    next_mask: np.ndarray | None = None,\n    learning_rate: float = 0.1,\n    discount_factor: float = 0.99,\n) -&gt; float:\n    \"\"\"Perform one 1-step TD (Q-learning) update.\n\n    Args:\n        q_table (np.ndarray): Q-value table with shape ``(num_states, num_actions)``.\n            The updated value is written in-place.\n        state_idx (int): Index of the current state $s_t$.\n        action_idx (int): Index of the action taken $a_t$.\n        reward (float): Immediate reward $r_{t+1}$.\n        next_state_idx (int): Index of the next state $s_{t+1}$.\n        next_mask (np.ndarray | None, optional): Binary mask for legal actions\n            in $s_{t+1}$ (``1``= legal, 0 = illegal). If *None*,\n            every action is considered legal.\n        learning_rate (float, optional): Step\u2011size $\\alpha$.\n            Defaults to ``0.1``.\n        discount_factor (float, optional): Discount factor $\\gamma$.\n            Defaults to ``0.99``.\n\n    Returns:\n        float: The **new** Q-function ``q_table[state_idx, action_idx]``.\n\n    Raises:\n        ValueError: If *next_mask* is provided but has no ``1`` entries, i.e.\n            there are no legal actions in the next state.\n    \"\"\"\n    # \u2500\u2500 1. Determine bootstrap value (td_target) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    next_max = None\n    if next_mask is None:\n       pass\n    else:\n        pass\n\n    td_target = None\n\n    # \u2500\u2500 2. Compute TD error and apply update \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    td_error = 0\n    q_table[state_idx, action_idx] = 0\n\n    return NotImplementedError\n\nmake_tester(td_update, td_update_cases)\n</pre> def td_update(     q_table: np.ndarray,     state_idx: int,     action_idx: int,     reward: float,     next_state_idx: int,     next_mask: np.ndarray | None = None,     learning_rate: float = 0.1,     discount_factor: float = 0.99, ) -&gt; float:     \"\"\"Perform one 1-step TD (Q-learning) update.      Args:         q_table (np.ndarray): Q-value table with shape ``(num_states, num_actions)``.             The updated value is written in-place.         state_idx (int): Index of the current state $s_t$.         action_idx (int): Index of the action taken $a_t$.         reward (float): Immediate reward $r_{t+1}$.         next_state_idx (int): Index of the next state $s_{t+1}$.         next_mask (np.ndarray | None, optional): Binary mask for legal actions             in $s_{t+1}$ (``1``= legal, 0 = illegal). If *None*,             every action is considered legal.         learning_rate (float, optional): Step\u2011size $\\alpha$.             Defaults to ``0.1``.         discount_factor (float, optional): Discount factor $\\gamma$.             Defaults to ``0.99``.      Returns:         float: The **new** Q-function ``q_table[state_idx, action_idx]``.      Raises:         ValueError: If *next_mask* is provided but has no ``1`` entries, i.e.             there are no legal actions in the next state.     \"\"\"     # \u2500\u2500 1. Determine bootstrap value (td_target) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500     next_max = None     if next_mask is None:        pass     else:         pass      td_target = None      # \u2500\u2500 2. Compute TD error and apply update \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500     td_error = 0     q_table[state_idx, action_idx] = 0      return NotImplementedError  make_tester(td_update, td_update_cases) <pre>&lt;&gt;:11: SyntaxWarning: invalid escape sequence '\\g'\n&lt;&gt;:11: SyntaxWarning: invalid escape sequence '\\g'\nC:\\Users\\Omar\\AppData\\Local\\Temp\\ipykernel_8496\\3557948597.py:11: SyntaxWarning: invalid escape sequence '\\g'\n  \"\"\"Perform one 1-step TD (Q-learning) update.\n</pre> Out[3]: <pre>VBox(children=(HBox(children=(Button(button_style='success', description='Run Tests', icon='check', style=Butt\u2026</pre>"},{"location":"2-Tabular-Q-Learning/#2-tabular-q-learning","title":"2. Tabular Q Learning\u00b6","text":""},{"location":"2-Tabular-Q-Learning/#the-control-problem","title":"The \"Control\" Problem\u00b6","text":"<p>A reinforcement\u2011learning agent interacts with an environment in a repeated loop:</p> <ol> <li><p>Observe &amp; act The agent observes its current state and chooses an action.</p> </li> <li><p>Environment transition The environment then returns</p> <ul> <li>the next state, drawn randomly from the transition probabilities conditioned on the current state and action, and</li> <li>a numerical reward value.</li> </ul> </li> <li><p>Repeat Control returns to Step 1 and the cycle continues until the episode terminates, either because an end-state (terminal state) is reached or because a predefined time step limit is exceeded.</p> </li> </ol> <p>Let us consider an example environment where a robot is learning to solve a maze. The environment could either be:</p> <ul> <li>Simulated \u2013 defined entirely by software. Modern simulators can step extremely fast, allowing you to run millions of time\u2011steps per second on a GPU cluster.</li> <li>Real\u2011world \u2014 the physical maze itself with a robot that is able to detect its state, select an action, and receive a reward. Here the interaction rate is limited by actuator and sensor latency, often on the order of one action per second. To collect experience faster you would need to run multiple robots in parallel.</li> </ul> <p>Because the simulated environment is faster to train on, you would likely want to train the robot on the simulated environment that replicates the real-world environment, and later deploy the robot in the real-world rather than training there. However, perfect replication remains a challenge, and this is called the \"sim-to-real\" challenge.</p> <p>In both cases the formal control loop above remains the same: observe, act, receive the next state and reward, repeat. You can optionally read the following subsection for learning more theoretical background on modelling the agent-environment interaction.</p>"},{"location":"2-Tabular-Q-Learning/#optional-background-markov-decision-processes","title":"Optional Background) Markov Decision Processes\u00b6","text":"<p>We can formally model the agent-environment interaction contract as a Markov Decision Process (MDP). This modelling helps us rigorously and clearly define the problem and our assumptions.</p> <p>A Markov Decision Process $\\mathcal{M}$ is described by the elements in the following tuple: $$\\mathcal M = \\langle \\mathcal S,\\;\\mathcal A,\\;P,\\;R,\\;\\gamma\\rangle$$</p> <ul> <li>States where each state $s \\in \\mathcal{A}$ captures the active facts of the system.</li> <li>Actions in each state $s$ ($s \\in \\mathcal{S}$), an agent can take an action $a (a \\in \\mathcal{A}$), which results in a new state $s'$ that is the result of taking the action. Actions can either be deterministic (always ending up in the same resulting state) or probabilistic (ending up in a certain state from a set of states due to stochastic/random/uncertain elements).</li> <li>$P(s' \\mid s,a)$ is the transition probability from a starting state $s$, taking a particular action $a$, to the next state $s'$.</li> <li>$R(s,a,s') = \\mathbb E\\![\\,r_{t+1}\\mid s_t=s,\\,a_t=a,\\,s_{t+1}=s'\\,]$ is the expected one-step reward. One-step is defined by the agent starting in state $s$, takes an action $a$, and ends up in the resultant state $s'$.</li> <li>$\\gamma\\in[0,1]$ discounts future rewards</li> </ul> <p>The return can be defined as $G_t$, as below: $$ G_t \\;=\\; r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots $$</p> <p>The above definitions are quite straightforward, however, you may be wondering what the purpose of a discount factor exactly is. A low discount factor (e.g 0.1) essentially states that getting a smaller reward earlier is better than a large reward much later down the line. In non-episodic RL settings, since the reward horizon may be infinite, you lose convergence guarantees without setting a discount factor. Whereas in episodic RL settings, while you can use a discount factor of 1.0, a slightly smaller $\\gamma$ (e.g. 0.99) damps far-future noise, helping reduce variance.</p>"},{"location":"2-Tabular-Q-Learning/#finding-an-optimal-policy","title":"Finding an Optimal Policy\u00b6","text":"<p>Recall that a policy is the rulebook that tells the agent which action to take in any particular state. A policy can be deterministic (one state maps to one action) or stochastic (one state maps to a probability distribution of actions to take). For now, we will assume a deterministic policy.</p> <p>Our goal is to find an optimal policy, which we call $\\pi^*$, that maximises the expected return from every state.</p>"},{"location":"2-Tabular-Q-Learning/#learning-with-model-based-and-model-free-algorithms","title":"Learning with Model-Based and Model-Free algorithms\u00b6","text":"<p>There are two key kinds of Reinforcement Learning algorithms we can consider, one is model-based algorithms, and the other is model-free algorithms. Model-based algorithms assume having access to a function which predicts state transitions ($* $P(s' \\mid s,a)$) and rewards ($R(s,a,s')$), which we call a model. Analogously, model-free algorithms do not assume access to such a predicting function.</p> <p>If you have a model, you can perform planning (i.e. \"thinking\" multiple time steps ahead before performing actions), which in turn increases the sample efficiency of learning (meaning that you do not have to perform as much training). This model can also be learnt from interacting with the environment, with the challenge of learning an accurate representation, particularly with unseen data. However, the real environment dynamics are usually not accessible to the agent or can be costly to compute (e.g. involves calling an external solver). Additionally, vanilla planning techniques are not as suited towards large combinatorial action and state spaces. As model-free algorithms involve a weaker assumption, and thus is more general to learn, we will start with a model-free algorithm known as Tabular Q-Learning.</p>"},{"location":"2-Tabular-Q-Learning/#tabular-q-learning","title":"Tabular Q-Learning\u00b6","text":"<p>Recall that we want to train our reinforcement\u2011learning (RL) agent so that it maximises the cumulative (discounted) return \u2014 the total reward it collects over time.</p> <p>A common way to achieve this is to learn an action\u2011value function $Q(s, a)$. From experience, we estimate how advantageous it is to take action $a$ in state $s$ and then follow the current policy thereafter.</p>"},{"location":"2-Tabular-Q-Learning/#the-bellman-recursive-relationship","title":"The Bellman Recursive Relationship\u00b6","text":"<p>The optimal action\u2011value function\u00a0$Q^*$ satisfies the Bellman optimality equation: $$Q^*(s,a)=\\mathbb E_{s'}\\bigl[R_{t+1}+\\gamma\\max_{a'}Q^*(s',a') \\,\\big|\\, s_t=s,\\ a_t=a\\bigr]$$</p> <p>Because\u00a0$Q^*$ on the right\u2011hand side is the very function we are trying to learn, the update bootstraps off its own, imperfect estimate of the next state. In practice we do not know the expectation over future states and rewards; we only ever see single samples $(s,a,r,s')$. Each update therefore uses</p> <ul> <li>a sampled immediate reward\u00a0$R_{t+1}$, and</li> <li>the current table entry\u00a0$\\max_{a'}Q(S_{t+1},a')$ as a proxy for the (unknown) optimal value of the next state.</li> </ul> <p>Any error in the estimate of the next state propagates backwards through the recursion, which can make learning unstable if the learning\u2011rate\u00a0$\\alpha$ is too high or the policy does not visit states often enough. Nevertheless, the recursion is also what allows Q\u2011learning to back\u2011up information from sparse rewards without enumerating the entire state space.</p> <p>The one\u2011step tabular update is $$ \\underbrace{Q(S_t,A_t)}_{\\text{new Q\u2011value estimate}} \\;\\leftarrow\\; \\underbrace{Q(S_t,A_t)}_{\\text{former Q\u2011value estimate}} \\;+\\; \\underbrace{\\alpha}_{\\text{learning rate}} \\, \\overbrace{\\Bigl[       \\overbrace{         \\underbrace{R_{t+1}}_{\\text{immediate reward}}         \\;+\\;         \\underbrace{\\gamma\\max\\limits_{a} Q(S_{t+1},a)}_{\\text{discounted estimate of next state}}       }^{\\text{TD target}}       \\;-\\;       \\underbrace{Q(S_t,A_t)}_{\\text{former Q\u2011value estimate}}     \\Bigr]}^{\\text{TD error}} $$</p>"},{"location":"2-Tabular-Q-Learning/#exploration-exploitation","title":"Exploration \u2194 Exploitation\u00b6","text":"<p>In Reinforcement Learning, there is a classical dilemma know as the exploration-exploitation trade-off. At each step, the agent must choose between:</p> <ul> <li>exploitation\u00a0\u2014 taking the action it currently believes has the highest value so as to harvest reward now, and</li> <li>exploration\u00a0\u2014 trying actions that might be worse now but could reveal better long\u2011term returns.</li> </ul> <p>A simple (and still very effective) strategy is $\\varepsilon$\u2011greedy exploration:</p> <ul> <li>with probability $1-\\varepsilon$ select the greedily optimal action $a^*=\\arg\\max_a Q(s,a)$ (exploit);</li> <li>with probability $\\varepsilon$ pick a random action (explore).</li> </ul> <p>Typical training schedules start with $\\varepsilon\\approx0.1\\text{\u2013}1.0$ to encourage wide exploration and decay $\\varepsilon$ slowly toward\u00a00 as the table becomes more reliable.</p> <p>More sophisticated alternatives (e.g. Boltzmann exploration or Upper\u2011Confidence\u2011Bound methods) adapt the amount of exploration per state, but $\\varepsilon$\u2011greedy is often sufficient for tabular problems.</p>"},{"location":"2-Tabular-Q-Learning/#action-masking","title":"Action masking\u00b6","text":"<p>When sampling actions, a policy ordinarily considers the full discrete action space $\\mathcal A$, regardless of the current state $s$. In many tasks only a subset $A(s)\\subseteq \\mathcal A$ is actually legal in state $s$.</p> <p>For example, imagine a 1\u2011D bin\u2011packing environment where items arrive sequentially and must be placed into one of $k$ fixed\u2011capacity bins. By having a mask, you can prevent the agent from performing an illegal action, that is placing an item in an already filled bin.</p>"},{"location":"2-Tabular-Q-Learning/#optional-background-on-policy-vs-off-policy","title":"Optional Background) On-policy vs Off-policy\u00b6","text":"<p>A policy is the rule an agent follows to choose actions\u2014e.g. an \u03b5\u2011greedy rule, a softmax distribution or the output of a neural network.</p> <p>In practice two distinct policies matter:</p> Policy Role in learning Behaviour policy Interacts with the environment and produces the experience stored in memory Target policy The policy you attempt to improve when updating value estimates or network weights <p>The following table provides an overview of on-policy vs off-policy algorithms. In on-policy algorithms, the data considered must be gathered by the current policy.</p> Aspect On\u2011policy Off\u2011policy Source of data Must come from the current policy Can come from any policy (earlier checkpoints, exploratory variants, a random policy, \u2026) Key advantage Learning dynamics are stable and easy to reason about High sample\u2011efficiency because past experience can be re\u2011used Key drawback Discards data after each update (wasteful) Risk of divergence if the replay distribution drifts too far from the target policy Canonical algorithms SARSA, REINFORCE, PPO Q\u2011Learning, DQN, SAC, TD3 <p>Tabular Q\u2011learning is off\u2011policy: the agent behaves \u03b5\u2011greedy (behaviour policy) but each update targets the greedy action implied by the current Q\u2011table (target policy).</p> <p>Although off\u2011policy methods dominate when sample efficiency is critical, on\u2011policy algorithms remain popular when stability and predictable convergence are higher priorities. Some of these considerations are only apparent in non-tabular settings. The key thing to note for you at this stage, is that on-policy algorithms restrict updates to using data from the current policy. Even a slight change to the policy invalidates old data.</p>"},{"location":"2-Tabular-Q-Learning/#programming-tabular-q-learning","title":"Programming Tabular Q-Learning\u00b6","text":"<p>Now that we have all the foundational knowledge required for Tabular Q-Learning, we can now try to code up Tabular Q-Learning. This will be in exercise-based format, as shown in Notebook 1.</p>"},{"location":"2-Tabular-Q-Learning/#varepsilon-greedy-exploration","title":"$\\varepsilon-greedy$ exploration\u00b6","text":"<p>Let us implement our exploration strategy in the following exercise.</p> <ol> <li>We must choose when to exploit or explore, based on a probability</li> <li>We must constraint ourselves to a legal action set</li> </ol>"},{"location":"2-Tabular-Q-Learning/#one-step-temporal-difference-update-td-update","title":"One step Temporal Difference update (TD update)\u00b6","text":"<p>Next, we shall implement the TD update, according to the following equation we have seen prior:</p> <p>$$ \\underbrace{Q(S_t,A_t)}_{\\text{new Q\u2011value estimate}} \\;\\leftarrow\\; \\underbrace{Q(S_t,A_t)}_{\\text{former Q\u2011value estimate}} \\;+\\; \\underbrace{\\alpha}_{\\text{learning rate}} \\, \\overbrace{\\Bigl[       \\overbrace{         \\underbrace{R_{t+1}}_{\\text{immediate reward}}         \\;+\\;         \\underbrace{\\gamma\\max\\limits_{a} Q(S_{t+1},a)}_{\\text{discounted estimate of next state}}       }^{\\text{TD target}}       \\;-\\;       \\underbrace{Q(S_t,A_t)}_{\\text{former Q\u2011value estimate}}     \\Bigr]}^{\\text{TD error}} $$</p>"},{"location":"2-Tabular-Q-Learning/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we covered the Reinforcement Learning control problem, with all of the nuances involved, and how to solve it using the most well-known and foundational approach, Tabular Q learning.</p> <p>We implemented two key functions for Tabular Q Learning, that is, how the exploration-exploitation using $\\varepsilon$ -greedy and how the Q-value table is updated after each time step, demonstrating your understanding of all of the details listed above. In future notebooks, we will import a provided reference implementation of Tabular Q Learning, which uses the same logic as here but has all the required helper/training-loop functions. You can view this reference implementation in more detail at <code>src/rl/tabular_q_learning.py</code> of the repository, but the important concepts have already been implemented in this notebook.</p> <p>Solutions to the exercises are also available at <code>notebooks/solutions/tabular_q_learning.py</code> in case you are stuck in some implementation detail or perhaps you would simply like to compare your code.</p> <p>In the next notebook, we will explore how to train and visualise your algorithm in detail, including using hyperparameter tuning rather than trying to guess the optimal learning rate and epsilon yourself!</p>"},{"location":"2-Tabular-Q-Learning/#extensions","title":"Extensions\u00b6","text":"<p>Below are optional extensions to reinforce your understanding of everything covered in this notebook. The above material is sufficient for advancing to the subsequent notebooks, and some of these extensions are quite challenging and time consuming.</p> <p>You can also simply revisit these extensions after Notebook 5.</p>"},{"location":"2-Tabular-Q-Learning/#extension-1-the-markovian-property","title":"Extension 1. The Markovian property\u00b6","text":"<p>For this task, you will find it useful to read the subsection \"Optional Background) Markov Decision Processes\"</p> <p>A Markov Decision Process (MDP) is named so because it follows the Markovian property. Explain what the Markovian property is, and provide an example environment where it might be natural to use Non-Markovian Reward Decision Processes.</p>"},{"location":"2-Tabular-Q-Learning/#extension-2-boltzmann-exploration-strategy","title":"Extension 2. Boltzmann Exploration Strategy\u00b6","text":"<p>Unlike \u03b5\u2011greedy, which picks the current best action with probability 1\u202f\u2212\u202f\u03b5 and otherwise explores uniformly, Boltzmann (soft\u2011max) exploration samples actions in proportion to the exponentiated Q\u2011values.</p> <p>The temperature\u00a0\u03c4 (&gt;0) controls how sharp the distribution is: high \u03c4 \u2248 uniform; low \u03c4 \u2192 greedy.</p> <p>In this exercise, replace the exploration/action-selection mechanism in <code>src/rl/tabular_q_learning.py</code> with one based on Boltzmann exploration, which often performs better in practice.</p> <p>Suggested resources</p> <ul> <li>Sutton &amp;\u00a0Barto, Reinforcement Learning (2nd\u202fed.), \u00a72.8 \u201cGradient Bandit Algorithms\u201d</li> </ul>"},{"location":"2-Tabular-Q-Learning/#extension-3-implement-an-on-policy-algorithm","title":"Extension 3. Implement an On-Policy Algorithm\u00b6","text":"<p>For this task, you will find it useful to read the subsection \"Optional Background) On-policy vs Off-policy\" This is a difficult extension, and can be done after completing the course.</p> <p>Our Tabular Q-Learning algorithm is an off-policy algorithm. Consider the algorithm <code>SARSA</code>, which is an on-policy algorithm.  You should be able to modify the algorithm provided in <code>src/rl/tabular_learning.py</code>, updating the TD-target.</p> <p>$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha\\bigl[r + \\gamma Q(s',a') - Q(s,a)\\bigr]. $$</p> <p>Replace the target in <code>td_update</code> and train with the same \u03b5\u2011greedy policy. Compare learning curves with Q\u2011learning after Notebook 4; SARSA often converges more safely in stochastic domains. For longer\u2011horizon credit assignment, extend to SARSA(\u03bb) with eligibility traces.</p> <p>Suggested resources</p> <ul> <li>Sutton &amp;\u00a0Barto, \u00a76.4 \u201cSarsa: On\u2011policy TD Control\"</li> <li>Csaba\u00a0Szepesv\u00e1ri, Algorithms for RL, \u00a74.4.1</li> </ul>"},{"location":"2-Tabular-Q-Learning/#glossary","title":"Glossary\u00b6","text":"<p>This will be collated with all notebooks and definitions then updated based on what appears in each notebook. Conclusion will have the full glossary.</p> <ul> <li>agent: the learner/decision\u2011maker that observes the state, selects actions via a policy, receives rewards, and updates its knowledge to maximize return.</li> <li>action mask: constraints the action space to only allowing the agent to randomly sample the valid, legal actions (within the rules of the environment) for that particular time step. In some environments, all actions from the action space are always allowed, and thus no mask is needed. Although, very often, an action mask is needed in combinatorial environments.</li> <li>discount factor ($\\gamma$): a number in $[0,1]$ that down\u2011weights future rewards when computing return. $\\gamma$=0 cares only about immediate reward; $\\gamma$ close to 1 values long\u2011term reward.</li> <li>Markov decision process (MDP): a formal model of the environment defined by states $\\mathcal{S}$, actions $\\mathcal{A}$, transition dynamics $P$, reward function $R$, and discount $\\gamma$.</li> </ul> <ul> <li>on\u2011policy: learning about the value of the policy you're actually using to act (the behaviour policy). Example: SARSA updates toward what the agent actually did next.</li> <li>off\u2011policy: learning about a (possibly optimal) target policy while behaving with a different policy for exploration. Classic example: Q\u2011learning updates toward the greedy action's value (max over actions) even if it behaved $\\varepsilon$\u2011greedily.</li> <li>policy ($\\pi$): a mapping from states to actions (deterministic) or to a probability distribution over actions (stochastic), written $\\pi(a \\mid s)$.</li> <li>scalar: a 1 dimensional vector, a single numerical value.</li> <li>return ($G_t$): is the sum of all rewards obtained, either in a finite window number of time steps, or an infinite-horizon return where all the rewards ever obtained by the agent is considered. The return is said to be a discounted return if used with a discount factor $\\gamma$.</li> <li>Q\u2011table: a table with one entry per (state, action) pair storing the current estimate of its Q\u2011value. In Tabular Q\u2011Learning it\u2019s typically a $|S| \\times |A|$ array updated as experience is collected.</li> <li>Q\u2011value: the expected discounted return starting from state $s$, taking action $a$, then following a policy thereafter. For the optimal Q\u2011value, $Q^*(s,a)$ uses the optimal policy. Q\u2011function: the function $Q(s,a)$ that maps any (state, action) to its Q\u2011value; in tabular methods it\u2019s represented by the Q\u2011table. Also see the Bellman optimality relation.</li> </ul>"},{"location":"3-Experiments/","title":"3. Experiments","text":"In\u00a0[\u00a0]: Copied! <pre>from environments import MontyHallEnv, MontyHallDiscreteWrapper\nfrom src.rl import QLearningConfig, QLearningAgent\n\nimport os\nimport gymnasium as gym\nimport wandb\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> from environments import MontyHallEnv, MontyHallDiscreteWrapper from src.rl import QLearningConfig, QLearningAgent  import os import gymnasium as gym import wandb import numpy as np import matplotlib.pyplot as plt <p>Overall in this notebook, our aim is to learn whether it is optimal or not to actually switch or remain. We can see this by rolling out our policy.</p> <p>You can also play this environment yourself in the notebook: <code>notebooks/extra/play_monty_hall.ipynb</code> before proceeding here.</p> In\u00a0[\u00a0]: Copied! <pre># 1) Let us create a default config and default Q Learning Agent\nconfig = QLearningConfig(\n    learning_rate=0.1,\n    discount_factor=0.99,\n    epsilon_start=1.0,\n    epsilon_decay=0.9985,\n    epsilon_min=0.05,\n    max_steps_per_episode=100,\n    video_dir=\"exports/monty_hall\",\n)\n\n# 2) Create an instance of the Monty Hall environment\nenv = gym.make(\n    \"MontyHall-v0\",\n    n_doors=3,\n    n_cars=1,\n    render_mode=\"rgb_array\",\n)\n\n# 2a) Tabular Q Learning requires a Discrete action space: not MultiDiscrete\nenv = MontyHallDiscreteWrapper(env)\n\n# 3) An instance of Tabular Q Learning\nagent = QLearningAgent(env, config)\n\n# 4) Train for 500 episodes\nnum_episodes = 500\nepisode_rewards, converged_episode, converged_time = agent.train(\n    episodes=num_episodes,\n    log_interval=10,\n)\n\n# 4) Print out what happened each episode\nfor i, r in enumerate(episode_rewards, start=1):\n    print(f\"Episode {i:2d} \u2192 Reward: {r:+.1f}\")\n\nif converged_episode != -1:\n    print(f\"\\nConverged at episode {converged_episode} after {converged_time:.2f}s\")\nelse:\n    print(\"\\nDid not converge within 10 episodes.\")\n\n# 5) See the policy\nprint(\"\\nRunning 10 test episodes with the trained policy:\")\nagent.test(episodes=10)\n\nenv.close()\n</pre> # 1) Let us create a default config and default Q Learning Agent config = QLearningConfig(     learning_rate=0.1,     discount_factor=0.99,     epsilon_start=1.0,     epsilon_decay=0.9985,     epsilon_min=0.05,     max_steps_per_episode=100,     video_dir=\"exports/monty_hall\", )  # 2) Create an instance of the Monty Hall environment env = gym.make(     \"MontyHall-v0\",     n_doors=3,     n_cars=1,     render_mode=\"rgb_array\", )  # 2a) Tabular Q Learning requires a Discrete action space: not MultiDiscrete env = MontyHallDiscreteWrapper(env)  # 3) An instance of Tabular Q Learning agent = QLearningAgent(env, config)  # 4) Train for 500 episodes num_episodes = 500 episode_rewards, converged_episode, converged_time = agent.train(     episodes=num_episodes,     log_interval=10, )  # 4) Print out what happened each episode for i, r in enumerate(episode_rewards, start=1):     print(f\"Episode {i:2d} \u2192 Reward: {r:+.1f}\")  if converged_episode != -1:     print(f\"\\nConverged at episode {converged_episode} after {converged_time:.2f}s\") else:     print(\"\\nDid not converge within 10 episodes.\")  # 5) See the policy print(\"\\nRunning 10 test episodes with the trained policy:\") agent.test(episodes=10)  env.close() In\u00a0[\u00a0]: Copied! <pre>dpi     = 120       # figure resolution\n\nr = np.asarray(episode_rewards, dtype=float)\n\ncumsum = np.cumsum(np.insert(r, 0, 0))\nsmooth = (cumsum[num_episodes:] - cumsum[:-num_episodes]) / num_episodes\nepisodes_smooth = np.arange(len(smooth)) + num_episodes//2 + 1\n\nplt.figure(figsize=(10, 5), dpi=dpi)\nplt.plot(range(1, len(r) + 1), r, alpha=0.25, label='Episode reward')\n\nplt.title('Training Performance: Episode Rewards', fontsize=14)\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(frameon=False)\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> dpi     = 120       # figure resolution  r = np.asarray(episode_rewards, dtype=float)  cumsum = np.cumsum(np.insert(r, 0, 0)) smooth = (cumsum[num_episodes:] - cumsum[:-num_episodes]) / num_episodes episodes_smooth = np.arange(len(smooth)) + num_episodes//2 + 1  plt.figure(figsize=(10, 5), dpi=dpi) plt.plot(range(1, len(r) + 1), r, alpha=0.25, label='Episode reward')  plt.title('Training Performance: Episode Rewards', fontsize=14) plt.xlabel('Episode') plt.ylabel('Reward') plt.legend(frameon=False) plt.grid(alpha=0.3) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>wandb.login() # See https://docs.wandb.ai/quickstart/\n</pre> wandb.login() # See https://docs.wandb.ai/quickstart/ In\u00a0[\u00a0]: Copied! <pre>def train_sweep() -&gt; None:\n    \"\"\"  One full training run driven by a set of hyper-parameters coming\n    from wandb.config. Logs metrics to W&amp;B at the end of every episode.\n    \"\"\"\n    with wandb.init(project=\"montyhall-ql\") as run:\n        cfg = run.config\n\n        # 1. Converting the Weight &amp; Biases Config -&gt; Q Learning Config\n        qcfg = QLearningConfig(\n            learning_rate   = cfg.learning_rate,\n            discount_factor = cfg.discount_factor,\n            epsilon_start   = 1.0,\n            epsilon_decay   = cfg.epsilon_decay,\n            epsilon_min     = 0.05,\n            max_steps_per_episode = 100,\n            video_dir       = None\n        )\n\n        # 2\ufe0f) Create env &amp; agent\n        env  = gym.make(\"MontyHall-v0\", n_doors=3, n_cars=1, render_mode=None)\n        env  = MontyHallDiscreteWrapper(env)\n        agent = QLearningAgent(env, qcfg)\n\n        # 3\ufe0f) Run training loop\n        episode_rewards, converged_ep, converged_t = agent.train(\n            episodes     = 500,\n            log_interval = 25,\n        )\n\n        # 4\ufe0f) Log summary metrics &amp; clean up\n        wandb.log({\n            \"best_reward\"      : max(episode_rewards),\n            \"converged_episode\": converged_ep,\n            \"train_time_sec\"   : converged_t,\n        })\n        env.close()\n</pre> def train_sweep() -&gt; None:     \"\"\"  One full training run driven by a set of hyper-parameters coming     from wandb.config. Logs metrics to W&amp;B at the end of every episode.     \"\"\"     with wandb.init(project=\"montyhall-ql\") as run:         cfg = run.config          # 1. Converting the Weight &amp; Biases Config -&gt; Q Learning Config         qcfg = QLearningConfig(             learning_rate   = cfg.learning_rate,             discount_factor = cfg.discount_factor,             epsilon_start   = 1.0,             epsilon_decay   = cfg.epsilon_decay,             epsilon_min     = 0.05,             max_steps_per_episode = 100,             video_dir       = None         )          # 2\ufe0f) Create env &amp; agent         env  = gym.make(\"MontyHall-v0\", n_doors=3, n_cars=1, render_mode=None)         env  = MontyHallDiscreteWrapper(env)         agent = QLearningAgent(env, qcfg)          # 3\ufe0f) Run training loop         episode_rewards, converged_ep, converged_t = agent.train(             episodes     = 500,             log_interval = 25,         )          # 4\ufe0f) Log summary metrics &amp; clean up         wandb.log({             \"best_reward\"      : max(episode_rewards),             \"converged_episode\": converged_ep,             \"train_time_sec\"   : converged_t,         })         env.close() In\u00a0[\u00a0]: Copied! <pre>sweep_config = dict(\n    name   = \"montyhall-bayes\",\n    method = \"bayes\",                      # The search method used: grid | random | bayes. Random &amp; Bayes work best.\n    metric = dict(name=\"best_reward\", goal=\"maximize\"),\n    parameters = dict(\n        learning_rate   = dict(min=1e-5,  max=1e-2,  distribution=\"log_uniform_values\"),\n        discount_factor = dict(min=0.8,   max=0.999, distribution=\"uniform\"),\n        epsilon_decay   = dict(min=0.95,  max=0.9999,distribution=\"uniform\"),\n    ),\n    early_terminate = dict(type=\"hyperband\", min_iter=10),\n)\n\nsweep_id = wandb.sweep(sweep_config, project=\"montyhall-ql\")\nwandb.agent(sweep_id, function=train_sweep, count=30) # 30 trials\n</pre> sweep_config = dict(     name   = \"montyhall-bayes\",     method = \"bayes\",                      # The search method used: grid | random | bayes. Random &amp; Bayes work best.     metric = dict(name=\"best_reward\", goal=\"maximize\"),     parameters = dict(         learning_rate   = dict(min=1e-5,  max=1e-2,  distribution=\"log_uniform_values\"),         discount_factor = dict(min=0.8,   max=0.999, distribution=\"uniform\"),         epsilon_decay   = dict(min=0.95,  max=0.9999,distribution=\"uniform\"),     ),     early_terminate = dict(type=\"hyperband\", min_iter=10), )  sweep_id = wandb.sweep(sweep_config, project=\"montyhall-ql\") wandb.agent(sweep_id, function=train_sweep, count=30) # 30 trials <p></p> <p>Explore this dashboard fully for more information on how the tuning and sweeps went.</p> <p>We can programmatically print the best hyperparameters as below:</p> In\u00a0[\u00a0]: Copied! <pre>api   = wandb.Api()\nsweep = api.sweep(f\"{os.getenv(\"WANDB_ENTITY\")}/montyhall-ql/{sweep_id}\")\nbest  = max(sweep.runs, key=lambda r: r.summary.get(\"best_reward\", float(\"-inf\")))\n\n\n\n# \u2500\u2500 2. Rank runs: highest reward FIRST, then earliest convergence \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef sort_key(run):\n    # Reward: want *larger* \u2192 negate so larger becomes \u201csmaller\u201d for sort()\n    reward = -run.summary.get(\"best_reward\", float(\"-inf\"))\n    # Convergence episode: want *smaller*\n    conv_ep = run.summary.get(\"converged_episode\", float(\"inf\"))\n    return (reward, conv_ep)\n\nbest_run = min(sweep.runs, key=sort_key)\n\n# \u2500\u2500 3. Print the winner\u2019s details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(f\"\ud83c\udfc6  {best_run.name}\")\nprint(f\"   \u2022 best_reward       : {best_run.summary['best_reward']}\")\nprint(f\"   \u2022 converged_episode : {best_run.summary['converged_episode']}\")\nprint(f\"   \u2022 hyperparams:\", {k: best_run.config[k] for k in ('learning_rate',\n                                                    'discount_factor',\n                                                    'epsilon_decay')})\n</pre> api   = wandb.Api() sweep = api.sweep(f\"{os.getenv(\"WANDB_ENTITY\")}/montyhall-ql/{sweep_id}\") best  = max(sweep.runs, key=lambda r: r.summary.get(\"best_reward\", float(\"-inf\")))    # \u2500\u2500 2. Rank runs: highest reward FIRST, then earliest convergence \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def sort_key(run):     # Reward: want *larger* \u2192 negate so larger becomes \u201csmaller\u201d for sort()     reward = -run.summary.get(\"best_reward\", float(\"-inf\"))     # Convergence episode: want *smaller*     conv_ep = run.summary.get(\"converged_episode\", float(\"inf\"))     return (reward, conv_ep)  best_run = min(sweep.runs, key=sort_key)  # \u2500\u2500 3. Print the winner\u2019s details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 print(f\"\ud83c\udfc6  {best_run.name}\") print(f\"   \u2022 best_reward       : {best_run.summary['best_reward']}\") print(f\"   \u2022 converged_episode : {best_run.summary['converged_episode']}\") print(f\"   \u2022 hyperparams:\", {k: best_run.config[k] for k in ('learning_rate',                                                     'discount_factor',                                                     'epsilon_decay')})"},{"location":"3-Experiments/#3-experiments","title":"3. Experiments\u00b6","text":""},{"location":"3-Experiments/#environments-defined-with-gymnasium","title":"Environments defined with Gymnasium\u00b6","text":"<p>Recall that Reinforcement Learning needs an environment to act in. It takes actions in this environment, learning what the optimal action is for each environment state it observes based on a reward it receives. Environments are independent of the algorithm being used.</p> <p><code>Gymnasium</code> (https://gymnasium.farama.org/index.html) is the most popular library for defining environment classes and provides a lot of helpful wrappers, as well as an unified and clearly defined interface that all environments follow. It is a fork (continuation) of OpenAI's <code>Gym</code>, note that the original <code>Gym</code> is no longer maintained by anyone. Sometimes, people refer to <code>Gym</code> when they mean <code>Gymnasium</code> as a shorthand, or in older posts/literature used <code>Gym</code> when it was the main library.</p> <p>Here, we intend to run the environment initially, to get a feel for how our implemented algorithm (Tabular Q Learning) works in practice with the Gymnasium interface on a provided environment, Monty Hall.</p>"},{"location":"3-Experiments/#monty-hall-environment","title":"Monty Hall environment\u00b6","text":"<p>The Monty Hall is a well-known problem that originated from a game show, it can be stated as follows:</p> <p>Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?</p>"},{"location":"3-Experiments/#creating-the-configuration-for-the-algorithm-an-agent-class-and-instantiating-the-environment","title":"Creating the configuration for the algorithm, an agent class, and instantiating the environment\u00b6","text":""},{"location":"3-Experiments/#visualisation-of-training","title":"Visualisation of Training\u00b6","text":"<p>Here, we want to visualise how well our training went in terms of the reward achieved throughout the episodes encountered by our training algorithm. The best way to do this is quite simple: plot the training so far.</p>"},{"location":"3-Experiments/#hyperparameter-tuning","title":"Hyperparameter tuning\u00b6","text":"<p>Some Reinforcement Learning algorithms have an abundance of parameters; especially in the territory of Deep Reinforcement Learning, where you could tune the batch size and even the neural network layout, amongst the learning rate and exploration rate.</p> <p>Therefore, it is important that we are able to systematically tune these hyperparameters in an automated fashion, as the number of combinations of parameters and interdependent effects are too numerous to manually tune (aside from setting a good starting point). Tuning is an art in and of itself, and is a form of optimization. Luckily, many libraries exist for finding the optimal set of hyperparameters, using various different approaches such as genetic algorithms, in order to explore and exploit areas of the tuning state space that are promising automatically.</p>"},{"location":"3-Experiments/#how-does-it-work","title":"How does it work?\u00b6","text":"<p>Exactly how hyperparameter tuning chooses to select combinations of parameters, continue runs (also known as sweeps, each sweep having an unique combination of parameters) or stop early, is an art in and of itself. There are many methods and algorithms that are out of scope for this project, although it should be noted that we are not simply enumerating all possible parameters as in a grid search, for example instead we can choose to use a random search or Bayes search.</p> <p>What we define is a tunable parameter, such as the following:</p> <pre>        learning_rate   = dict(min=1e-5,  max=1e-2,  distribution=\"log_uniform_values\"),\n</pre> <ul> <li><p>What it is: a rule for the hyperparameter tuning library, stating it should try out different learning-rates during a hyperparameter sweep.</p> </li> <li><p>The range: it may pick anything between 0.00001 and 0.01.</p> </li> <li><p>How the picks are made: not evenly spaced, but log-uniformly.</p> <ul> <li>Think \u201cpick a random exponent\u201d rather than \u201cpick a random number.\u201d</li> <li>Each order of magnitude (10\u207b\u2075 \u2192 10\u207b\u2074 \u2192 10\u207b\u00b3 \u2192 10\u207b\u00b2) is equally likely.</li> </ul> </li> </ul> <p>Why log-uniform?</p> <ol> <li>With learning rates, what matters is usually the order of magnitude, not the exact decimal.</li> <li>A log-uniform draw therefore spends just as much effort exploring tiny values (which often work best) as it does larger ones.</li> </ol> <p>In short: \u201cTry lots of learning-rates between 0.000 01 and 0.01, giving every magnitude the same chance, because we don\u2019t yet know which size step the optimiser will like.\u201d</p> <p>This is just one tunable parameter, and the problem of hyperparameter tuning is you have to select values for multiple hyperparameters at the same time, where hyperparameters are not always independent of each other, and you have limited compute time to run these experiments in. This is why specialised algorithms exist just for hyperparameter tuning.</p>"},{"location":"3-Experiments/#why-is-hyperparameter-tuning-needed","title":"Why is hyperparameter tuning needed?\u00b6","text":"<p>Finding the best set of hyperparameters allows converging (achieving the optimal/best performance) in a quicker fashion. It is possible that a default set of hyperparameters could converge to an optimal policy, but take 1000 times longer than using a set of optimal hyperparameters. Sometimes, without optimal hyperparameters it can also be easy for the policy to appear it has converged but it has converged to a sub-optimal policy instead of the policy that produces the best reward.</p>"},{"location":"3-Experiments/#weights-and-biases","title":"Weights and Biases\u00b6","text":"<p>Weights and Biases (referred to commonly as W&amp;B or WandB) is a standard tool used in both industry and academia for tracking (and storing) the results of experiments. In other words, Weights and Biases is an end-to-end developer platform that keeps every experiment, dataset, model checkpoint, and evaluation in a single, searchable workspace.</p> <p>Specifically, we use it both as a hyperparameter tuning library and storing the results of experiments based on these hyperparameters.</p> <p>You can find more information at: https://docs.wandb.ai/quickstart/</p>"},{"location":"3-Experiments/#viewing-the-results","title":"Viewing the results\u00b6","text":"<p>If you navigate to the sweep URL on Weights &amp; Biases (wandb.ai), you'll see the data logged in various formats such as table and graphs, where you can visualise the hyperparameter space. You should see something like the following:</p>"},{"location":"3-Experiments/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we learnt the following:</p> <ul> <li>Instantiating environments with the Gymnasium library</li> <li>Deployed our previously implemented Reinforcement Learning algorithm, Tabular Q Learning, on the aforementioned environment</li> <li>How to visualise training performance through plotting with Matplotlib</li> <li>How to perform hyperparameter tuning</li> <li>Followed best practices for experiment tracking and storage of results in an archival manner</li> </ul> <p>Following this notebook, we will learn the process of environment creation in Notebook 4. This allows solving any arbitrary combinatorial problem, allowing you to formally define a model of the \"problem space\" for any problem you can think of.</p>"},{"location":"3-Experiments/#extension","title":"Extension\u00b6","text":""},{"location":"3-Experiments/#extension-1-optimal-general-solution-to-monty-hall","title":"Extension 1. Optimal General Solution to Monty Hall\u00b6","text":"<p>As an extension to this, you can consider the following task:</p> <p>Based on our trained RL algorithm, using optimal hyperparameters above, how often is it optimal to switch or stay with the first selected door?</p> <p>You can do this by using the <code>.test()</code> method and collecting some basic statistics using the rolled-out policy.</p>"},{"location":"4-Creating-Envs-News-Vendor/","title":"4. Creating your own Environment: News Vendor","text":"In\u00a0[1]: Copied! <pre>from utils.grader import make_tester, run_tests\nfrom notebooks.internal.exercises_4_news_vendor import *\n</pre> from utils.grader import make_tester, run_tests from notebooks.internal.exercises_4_news_vendor import * Figure of the News Vendor Environment visualised <p>The classic single-product News Vendor problem is not considered a combinatorial-optimization problem; it is a continuous, single-variable stochastic optimization. However, many extensions of the problem, particularly those that add integer quantities, multiple items, or other discrete choices do turn it into a combinatorial problem. In this notebook, the focus is on the actual modelling of an environment in terms of the API - you can apply the same principles to combinatorial and non-combinatorial problems, and this is easy enough to model.</p> In\u00a0[2]: Copied! <pre>from dataclasses import dataclass\n\nimport numpy as np\nfrom collections.abc import Iterable\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom gymnasium.envs.registration import register\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n#                                 Configuration                                    #\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n@dataclass\nclass NewsVendorConfig:\n    # \u2500\u2500 Demand / system limits \u2500\u2500\n    lead_time: int = 5                 # periods before an order arrives\n    max_inventory: int = 4_000         # cap on pipeline\u202f+\u202fon\u2011hand units\n    max_order_quantity: int = 2_000    # upper bound for a single action\n\n    # \u2500\u2500 Cost parameters (upper bounds for random sampling) \u2500\u2500\n    max_sale_price: float = 100.0\n    max_holding_cost: float = 5.0\n    max_lost_sales_penalty: float = 10.0\n    max_demand_mean: float = 200.0              # Poisson mean upper bound\n\n    # \u2500\u2500 Episode &amp; discount \u2500\u2500\n    max_steps: int = 40\n    gamma: float = 1.0                 # discount on purchase cost\n\nclass NewsVendorEnv(gym.Env):\n    \"\"\"Gymnasium implementation of the classic multi-period news-vendor problem.\"\"\"\n\n    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n\n    def __init__(\n        self,\n        *,\n        config: NewsVendorConfig = NewsVendorConfig(),\n        render_mode: str | None = None,\n        seed: int | None = None,\n    ) -&gt; None:\n        \"\"\" A Gymnasium interface of the Multi-Period News Vendor with Lead Times from Balaji et. al.\n\n        The News Vendor problem is a seminal problem within inventory management, see: \n            Foundations of Inventory Management (Zipkin, 2000)\n\n        Inventory orders are not instantaneous and have multi-period leadtimes. There are costs inccured for holding\n        unsold inventory, although unsold inventory expires at the end of each period. There are also penalties\n        associated with losing goodwill by having unsold inventory.\n\n        Observation:\n            Type: Box\n            State Vector: S = (p, c, h, k, mu, x_l, x_l-1)\n            p = price\n            c = cost\n            h = holding cost\n            k = lost sales penalty\n            mu = mean of demand distribution\n            x_l = order quantities in the queue\n\n        Initial state:\n            Parameters p, c, h, k, and mu, with no inventory in the pipeline.\n\n        Actions:\n            Type: Box\n            Amount of product to order.\n\n        Reward:\n            Sales minus discounted purchase price, minus holding costs for\n            unsold product or penalties associated with insufficient inventory.\n\n        Episode termination:\n            By default, the environment terminates within 40 time steps.\n        \"\"\"\n        \n        self.config = config\n        self.render_mode = render_mode\n\n        # \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Observation space \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n        # state = [p, c, h, k, \u03bc, pipeline_1,  \u2026, pipeline_n]\n        #                          \u2191        \u2191\n        #                          newest   next to arrive\n        self.obs_dim = self.config.lead_time + 5\n        self.observation_space = spaces.Box(\n            low=np.zeros(self.obs_dim, dtype=np.float32),\n            high=np.array(\n                [self.config.max_sale_price,        # p\n                 self.config.max_sale_price,        # c  (bounded by price)\n                 self.config.max_holding_cost,      # h\n                 self.config.max_lost_sales_penalty,# k\n                 self.config.max_demand_mean]                # \u03bc\n                + [self.config.max_order_quantity] * self.config.lead_time,\n                dtype=np.float32,\n            ),\n        )\n\n        # \u2500\u2500\u2500 Action space \u2500\u2500\u2500 #\n        self.action_space = spaces.Box(\n            low=np.array([0.0], dtype=np.float32),\n            high=np.array([self.config.max_order_quantity], dtype=np.float32),\n        )\n\n        # \u2500\u2500\u2500 renderer (optional) \u2500\u2500\u2500\n        self._renderer = None\n\n        # \u2500\u2500\u2500 Initialisation \u2500\u2500\u2500 #\n        self.state = self.reset(seed=seed)\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n    #                                Private helpers                                   #\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n   \n    def _sample_economic_parameters(self) -&gt; None:\n        \"\"\"Draw random price/cost/penalties for a fresh episode.\"\"\"\n        self.price = max(1.0, self.np_random.random() * self.config.max_sale_price)\n            \n        # @Solve: sample \u223c\u202fU(0,\u202fprice) then lower\u2011bound at\u00a01.0\n        self.cost = ...   # \u2190 Unit purchase cost\u202f\u2264\u202fprice\n        \n        self.holding_cost_rate     = self.np_random.random() * min(self.cost, self.config.max_holding_cost)\n        \n        # @Solve: sample \u223c\u202fU(0,\u00a0max_lost_sales_penalty)\n        self.lost_sales_penalty = ...  # \u2190 goodwill\u2011loss penalty\n        \n        self.mu    = self.np_random.random() * self.config.max_demand_mean # mu = max demand mean\n    \n    def _reset_state(self) -&gt; np.ndarray:\n        \"\"\"Reset economic parameters and clear the pipeline.\"\"\"\n        self._sample_economic_parameters()\n        pipeline = np.zeros(self.config.lead_time, dtype=np.float32)\n        self.current_step = 0\n        return np.concatenate((\n            np.array([self.price, self.cost, self.holding_cost_rate, self.lost_sales_penalty, self.mu], dtype=np.float32),\n            pipeline,\n        ))\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n    #                                 Gymnasium API                                    #\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n    def reset(self, *, seed: int | None = None, options=None):\n        \"\"\"Resets the environment, whenever an episode has terminated or is beginning.\n\n        Args:\n            seed (int or None): reset the environment with a specific seed value. Defaults to None.\n            options: unused, mandated by the Gymnasium interface. Defaults to None.\n\n        Returns:\n            State vector\n        \"\"\"\n        super().reset(seed=seed)\n\n        self.state = self._reset_state()\n        return self.state\n\n    def step(self, action):\n        # \u2500\u2500 1:\u00a0Ensure scalar float \u2500\u2500\n        if isinstance(action, (np.ndarray, list)):\n            action = float(np.asarray(action).flatten()[0])\n        order_qty = np.clip(\n            action,\n            0.0,\n            min(\n                self.config.max_order_quantity,\n                self.config.max_inventory - self.state[5:].sum(),\n            ),\n        )\n\n        # \u2500\u2500 2:\u00a0Demand for this period \u2500\u2500\n        demand = self.np_random.poisson(lam=self.mu)\n\n        # \u2500\u2500 3:\u00a0Inventory available today \u2500\u2500\n        pipeline = self.state[5:]\n        inv_on_hand = order_qty if self.config.lead_time == 0 else pipeline[0]\n\n        # \u2500\u2500 4:\u00a0Sales outcomes \u2500\u2500\n        sales_revenue   = min(inv_on_hand, demand) * self.price\n        excess_inventory = max(0.0, inv_on_hand - demand)\n        short_inventory  = max(0.0, demand - inv_on_hand)\n\n        # \u2500\u2500 5:\u00a0Costs \u2500\u2500\n        purchase_cost      = order_qty * self.cost * (self.config.gamma ** self.config.lead_time)\n        # @Solve\n        holding_cost       = ...\n        # @Solve\n        lost_sales_penalty = ...\n\n        # \u2500\u2500 6:\u00a0Reward \u2500\u2500\n        reward = sales_revenue - purchase_cost - holding_cost - lost_sales_penalty\n        if isinstance(reward, Iterable):\n            reward = float(np.squeeze(reward))\n\n        # \u2500\u2500 7:\u00a0Advance pipeline \u2500\u2500\n        new_pipeline = np.zeros(self.config.lead_time, dtype=np.float32)\n        if self.config.lead_time &gt; 0:\n            new_pipeline[:-1] = pipeline[1:]\n            new_pipeline[-1]  = order_qty\n        self.state = np.hstack([self.state[:5], new_pipeline]).astype(np.float32)\n\n        # \u2500\u2500 8:\u00a0Episode termination \u2500\u2500\n        self.current_step += 1\n        terminated = self.current_step &gt;= self.config.max_steps\n        truncated  = False  # No time\u2011limit truncation beyond max_steps\n\n        return self.state, reward, terminated, truncated, {}\n\n    def render(self, mode: str | None = None):\n        if self.render_mode is None:\n            return None\n            \n        if not self._renderer:\n            raise ValueError(\"render_mode was None when env was created.\")\n        return self._renderer.render(self.state)\n\n    def close(self):\n        \"\"\"Closes the environment, performing some basic cleanup of resources.\"\"\"\n        if self._renderer:\n            self._renderer.close()\n            self._renderer = None\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n#               Register so users can call gymnasium.make()                        #\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nregister(\n    id=\"NewsVendor-v0\",\n    entry_point=\"environments.news_vendor.env:NewsVendorEnv\",\n    max_episode_steps=NewsVendorConfig().max_steps,\n)\n</pre> from dataclasses import dataclass  import numpy as np from collections.abc import Iterable import gymnasium as gym from gymnasium import spaces from gymnasium.envs.registration import register  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 # #                                 Configuration                                    # # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 # @dataclass class NewsVendorConfig:     # \u2500\u2500 Demand / system limits \u2500\u2500     lead_time: int = 5                 # periods before an order arrives     max_inventory: int = 4_000         # cap on pipeline\u202f+\u202fon\u2011hand units     max_order_quantity: int = 2_000    # upper bound for a single action      # \u2500\u2500 Cost parameters (upper bounds for random sampling) \u2500\u2500     max_sale_price: float = 100.0     max_holding_cost: float = 5.0     max_lost_sales_penalty: float = 10.0     max_demand_mean: float = 200.0              # Poisson mean upper bound      # \u2500\u2500 Episode &amp; discount \u2500\u2500     max_steps: int = 40     gamma: float = 1.0                 # discount on purchase cost  class NewsVendorEnv(gym.Env):     \"\"\"Gymnasium implementation of the classic multi-period news-vendor problem.\"\"\"      metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}      def __init__(         self,         *,         config: NewsVendorConfig = NewsVendorConfig(),         render_mode: str | None = None,         seed: int | None = None,     ) -&gt; None:         \"\"\" A Gymnasium interface of the Multi-Period News Vendor with Lead Times from Balaji et. al.          The News Vendor problem is a seminal problem within inventory management, see:              Foundations of Inventory Management (Zipkin, 2000)          Inventory orders are not instantaneous and have multi-period leadtimes. There are costs inccured for holding         unsold inventory, although unsold inventory expires at the end of each period. There are also penalties         associated with losing goodwill by having unsold inventory.          Observation:             Type: Box             State Vector: S = (p, c, h, k, mu, x_l, x_l-1)             p = price             c = cost             h = holding cost             k = lost sales penalty             mu = mean of demand distribution             x_l = order quantities in the queue          Initial state:             Parameters p, c, h, k, and mu, with no inventory in the pipeline.          Actions:             Type: Box             Amount of product to order.          Reward:             Sales minus discounted purchase price, minus holding costs for             unsold product or penalties associated with insufficient inventory.          Episode termination:             By default, the environment terminates within 40 time steps.         \"\"\"                  self.config = config         self.render_mode = render_mode          # \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Observation space \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e         # state = [p, c, h, k, \u03bc, pipeline_1,  \u2026, pipeline_n]         #                          \u2191        \u2191         #                          newest   next to arrive         self.obs_dim = self.config.lead_time + 5         self.observation_space = spaces.Box(             low=np.zeros(self.obs_dim, dtype=np.float32),             high=np.array(                 [self.config.max_sale_price,        # p                  self.config.max_sale_price,        # c  (bounded by price)                  self.config.max_holding_cost,      # h                  self.config.max_lost_sales_penalty,# k                  self.config.max_demand_mean]                # \u03bc                 + [self.config.max_order_quantity] * self.config.lead_time,                 dtype=np.float32,             ),         )          # \u2500\u2500\u2500 Action space \u2500\u2500\u2500 #         self.action_space = spaces.Box(             low=np.array([0.0], dtype=np.float32),             high=np.array([self.config.max_order_quantity], dtype=np.float32),         )          # \u2500\u2500\u2500 renderer (optional) \u2500\u2500\u2500         self._renderer = None          # \u2500\u2500\u2500 Initialisation \u2500\u2500\u2500 #         self.state = self.reset(seed=seed)      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #     #                                Private helpers                                   #     # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #         def _sample_economic_parameters(self) -&gt; None:         \"\"\"Draw random price/cost/penalties for a fresh episode.\"\"\"         self.price = max(1.0, self.np_random.random() * self.config.max_sale_price)                      # @Solve: sample \u223c\u202fU(0,\u202fprice) then lower\u2011bound at\u00a01.0         self.cost = ...   # \u2190 Unit purchase cost\u202f\u2264\u202fprice                  self.holding_cost_rate     = self.np_random.random() * min(self.cost, self.config.max_holding_cost)                  # @Solve: sample \u223c\u202fU(0,\u00a0max_lost_sales_penalty)         self.lost_sales_penalty = ...  # \u2190 goodwill\u2011loss penalty                  self.mu    = self.np_random.random() * self.config.max_demand_mean # mu = max demand mean          def _reset_state(self) -&gt; np.ndarray:         \"\"\"Reset economic parameters and clear the pipeline.\"\"\"         self._sample_economic_parameters()         pipeline = np.zeros(self.config.lead_time, dtype=np.float32)         self.current_step = 0         return np.concatenate((             np.array([self.price, self.cost, self.holding_cost_rate, self.lost_sales_penalty, self.mu], dtype=np.float32),             pipeline,         ))      # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #     #                                 Gymnasium API                                    #     # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #     def reset(self, *, seed: int | None = None, options=None):         \"\"\"Resets the environment, whenever an episode has terminated or is beginning.          Args:             seed (int or None): reset the environment with a specific seed value. Defaults to None.             options: unused, mandated by the Gymnasium interface. Defaults to None.          Returns:             State vector         \"\"\"         super().reset(seed=seed)          self.state = self._reset_state()         return self.state      def step(self, action):         # \u2500\u2500 1:\u00a0Ensure scalar float \u2500\u2500         if isinstance(action, (np.ndarray, list)):             action = float(np.asarray(action).flatten()[0])         order_qty = np.clip(             action,             0.0,             min(                 self.config.max_order_quantity,                 self.config.max_inventory - self.state[5:].sum(),             ),         )          # \u2500\u2500 2:\u00a0Demand for this period \u2500\u2500         demand = self.np_random.poisson(lam=self.mu)          # \u2500\u2500 3:\u00a0Inventory available today \u2500\u2500         pipeline = self.state[5:]         inv_on_hand = order_qty if self.config.lead_time == 0 else pipeline[0]          # \u2500\u2500 4:\u00a0Sales outcomes \u2500\u2500         sales_revenue   = min(inv_on_hand, demand) * self.price         excess_inventory = max(0.0, inv_on_hand - demand)         short_inventory  = max(0.0, demand - inv_on_hand)          # \u2500\u2500 5:\u00a0Costs \u2500\u2500         purchase_cost      = order_qty * self.cost * (self.config.gamma ** self.config.lead_time)         # @Solve         holding_cost       = ...         # @Solve         lost_sales_penalty = ...          # \u2500\u2500 6:\u00a0Reward \u2500\u2500         reward = sales_revenue - purchase_cost - holding_cost - lost_sales_penalty         if isinstance(reward, Iterable):             reward = float(np.squeeze(reward))          # \u2500\u2500 7:\u00a0Advance pipeline \u2500\u2500         new_pipeline = np.zeros(self.config.lead_time, dtype=np.float32)         if self.config.lead_time &gt; 0:             new_pipeline[:-1] = pipeline[1:]             new_pipeline[-1]  = order_qty         self.state = np.hstack([self.state[:5], new_pipeline]).astype(np.float32)          # \u2500\u2500 8:\u00a0Episode termination \u2500\u2500         self.current_step += 1         terminated = self.current_step &gt;= self.config.max_steps         truncated  = False  # No time\u2011limit truncation beyond max_steps          return self.state, reward, terminated, truncated, {}      def render(self, mode: str | None = None):         if self.render_mode is None:             return None                      if not self._renderer:             raise ValueError(\"render_mode was None when env was created.\")         return self._renderer.render(self.state)      def close(self):         \"\"\"Closes the environment, performing some basic cleanup of resources.\"\"\"         if self._renderer:             self._renderer.close()             self._renderer = None  # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 # #               Register so users can call gymnasium.make()                        # # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 # register(     id=\"NewsVendor-v0\",     entry_point=\"environments.news_vendor.env:NewsVendorEnv\",     max_episode_steps=NewsVendorConfig().max_steps, ) In\u00a0[3]: Copied! <pre># Check your code here - don't modify this cell!\ndef economic_parameters(seed: int):\n    \"\"\"Return (p, c, h, k, \u03bc) after env.reset(seed).\"\"\"\n    env = NewsVendorEnv()\n    env.reset(seed=seed)\n    return (env.price,\n            env.cost,\n            env.holding_cost_rate,\n            env.lost_sales_penalty,\n            env.mu)\n\ndef first_step_reward(seed: int, action: float):\n    \"\"\"Reward obtained from the very first step after env.reset(seed).\"\"\"\n    env = NewsVendorEnv()\n    env.reset(seed=seed)\n    _, reward, *_ = env.step(action)\n    return reward\n\nmake_tester(economic_parameters, economic_parameter_cases)\nmake_tester(first_step_reward, first_step_reward_cases)\n</pre> # Check your code here - don't modify this cell! def economic_parameters(seed: int):     \"\"\"Return (p, c, h, k, \u03bc) after env.reset(seed).\"\"\"     env = NewsVendorEnv()     env.reset(seed=seed)     return (env.price,             env.cost,             env.holding_cost_rate,             env.lost_sales_penalty,             env.mu)  def first_step_reward(seed: int, action: float):     \"\"\"Reward obtained from the very first step after env.reset(seed).\"\"\"     env = NewsVendorEnv()     env.reset(seed=seed)     _, reward, *_ = env.step(action)     return reward  make_tester(economic_parameters, economic_parameter_cases) make_tester(first_step_reward, first_step_reward_cases) Out[3]: <pre>VBox(children=(HBox(children=(Button(button_style='success', description='Run Tests', icon='check', style=Butt\u2026</pre>"},{"location":"4-Creating-Envs-News-Vendor/#4-creating-your-own-environment-news-vendor","title":"4. Creating your own Environment: News Vendor\u00b6","text":"<p>In this notebook, we will cover how you can create your own Reinforcement Learning environment that conforms to the Gymnasium interface/API provided by the Farama Foundation.</p>"},{"location":"4-Creating-Envs-News-Vendor/#why-should-we-conform-to-this-api","title":"Why should we conform to this API?\u00b6","text":"<p>An API (application\u2011programming interface) is simply a set of function signatures and data conventions: <code>reset()</code>, <code>step()</code>, <code>observation_space</code>, etc. that any compliant environment must expose. Think of it as the rules of the road that let agents and environments plug into each other without surprises.</p> <p>When your environment follows this contract, every agent implementation in the Gymnasium ecosystem (Stable\u2011Baselines\u202f3, CleanRL, RLlib, Ray Tune, etc.) can train on it instantly without any glue code nor hacky workarounds. Thus, you gain interoperability. Moreover, you gain free additional tooling by inheriting Gymnasium\u2019s wrappers, vectorized execution helpers, monitoring utilities, and visualizers, saving you from reinventing episode logging, video capture, or parallel rollouts.</p>"},{"location":"4-Creating-Envs-News-Vendor/#specifying-environments","title":"Specifying Environments\u00b6","text":"<p>Specifying an environment corresponds to specifying a Markov Decision Process, as you might have optionally read within Notebook 2. Without going into all the formal jargon and math, here's what we need to specify:</p> <ul> <li>State space: What are the possible states of the system we can be in, reached by taking actions from the initial state?</li> <li>Action space: What actions can be taken by the agent?<ul> <li>An action mask can also optionally be provided if applicable to the environment.</li> </ul> </li> <li>Transition function: if probabilistic transitions are involved from taking actions in states, then you must also provide the accompanying transition probabilities. You are in control of how the agent moves through the environment as you are the environment designer.</li> <li>Reward function: by convention, the environment provides a reward function that serves as some sort of ground truth. For example, in Monty Hall, if they gain a car we can provide a reward of 100, and if they gain a goat, we could provide a lesser reward of 10.<ul> <li>Users can override your reward function if they would like to try something else though just by implementing a wrapper: that's just a slice of the expressive power you gain by using Gymnasium!</li> </ul> </li> </ul>"},{"location":"4-Creating-Envs-News-Vendor/#news-vendor","title":"News Vendor\u00b6","text":"<p>Here, we are covering an environment called the News Vendor problem. While an implementation of this already exists in OR-gym, we will try to implement it from scratch to try to understand the environment creation process from scratch.</p> <p>The (multi\u2011period) News\u202fVendor problem models a trader who must decide how many perishable units to order before seeing demand that is random but statistically known. Ordering too much leaves leftover inventory that incurs holding or disposal costs, while ordering too little produces lost\u2011sales penalties or forgone margin; the optimal quantity balances these two expected costs at the critical ratio where the probability of selling an extra unit equals the ratio of under\u2011stocking cost to the sum of under\u2011 and over\u2011stocking costs.</p> <p>At the end of each period, stock that was not sold is \"expired\" (removed) and a holding penalty is applied for any excess stock. A goodwill penalty (shortage penalty) is applied if stock is not held despite it being in demand. Demand is probabilistically modelled. The agent observes stochastic price, cost, holding - and shortage\u2011penalty parameters plus an order\u2011pipeline vector, chooses a non\u2011negative order up to capacity each step, and receives profit (sales revenue minus discounted purchase, holding, and shortage costs).</p> <p>Therefore, this problem is a seminal problem that arises in inventory management, in terms of fast-moving consumer goods (FMCGs) and procurement.</p> <p>You are able to play the environment in <code>notebooks/extras/play_news_vendor.ipynb</code> if you would like to see how it works before proceeding, as it already comes with batteries attached (visualisation), see below for an example:</p>"},{"location":"4-Creating-Envs-News-Vendor/#conforming-to-gymnasiums-api","title":"Conforming to Gymnasium's API\u00b6","text":""},{"location":"4-Creating-Envs-News-Vendor/#spaces","title":"Spaces\u00b6","text":"<p>Think of each space as a contract that tells an RL agent (and you!) what valid actions and observations look like.</p> Space What it represents Quick constructor Typical use Tiny demo Box An n\u2011dimensional box of real or integer values (each dimension has its own lower/upper bound, possibly plus/minus infinity) Box(low, high, shape, dtype) Continuous actions (torques, joystick axes) or vector observations (positions, pixel stacks). <code>from gymnasium.spaces import Box; obs = Box(-1.0, 1.0, shape=(3,), dtype=float32); print(obs.sample())  # e.g. [-0.12  0.77 -0.55]</code> Discrete A finite set of consecutive integers {start, ..., start+n\u20111}. Discrete(n, start=0) Classic control actions (left/right), indexes into lookup tables. <code>action = Discrete(3)  # {0,1,2}; print(action.sample()) </code> MultiBinary A fixed\u2011shape binary array (0/1). MultiBinary(n) or MultiBinary((rows, cols)) Keyboards, on/off sensors, one\u2011hot vectors. <code>keys = MultiBinary(8); print(keys.sample())</code> MultiDiscrete Cartesian product of several independent Discrete spaces, each with its own range. MultiDiscrete([n0, n1, ...]) (or ndarray) Game\u2011controller combos: e.g. arrow\u2011keys x fire x jump. <code>pad = MultiDiscrete([5, 2, 2]); print(pad.sample())  # e.g. [3 0 1]</code> Text Variable\u2011length strings drawn from a character set. Text(max_len, min_len=1, charset=...) Natural\u2011language prompts, serial numbers, chat messages. <code>txt = Text(10); print(txt.sample())</code> <p>In this exemplar, we will only consider <code>Discrete</code> and <code>Box</code> spaces.</p> <p>Every space must have a known-size at initialization, e.g. <code>Discrete(n)</code>. For example, in Monty Hall, you can open one of three doors, so there's three actions. After revealing the first door, you can't choose a revealed door, but there are still 3 actions that can be constrained by action masking such to only allow the agent or user to pick only from the set of 2 valid actions.</p> <p>You can view more details in the Gymnasium documentation.</p>"},{"location":"4-Creating-Envs-News-Vendor/#the-contract-required","title":"The 'Contract' required\u00b6","text":"<p>Gymnasium also provides a couple of tutorials for creating a simple Gridworld environment here and a more complete tutorial with rendering.</p> <p>There are actually only four functions required to implement an environment as mandated by Gymnasium:</p> <ul> <li><p><code>step</code></p> <ul> <li><p>Signature:</p> <pre>def step(self, action: ActType) -&gt; tuple[ObsType, float, bool, bool, dict[str, Any]]\n</pre> </li> <li><p>Advances the environment one time\u2011step and returns the next observation, reward, <code>terminated</code>, <code>truncated</code>, and an <code>info</code> dictionary.</p> </li> </ul> </li> <li><p><code>reset</code></p> <ul> <li><p>Signature:</p> <pre>def reset(\n    self,\n    *,\n    seed: int | None = None,\n    options: dict[str, Any] | None = None\n) -&gt; tuple[ObsType, dict[str, Any]]\n</pre> </li> <li><p>Starts a new episode (optionally reseeding RNG and/or passing episode\u2011level options). Returns the initial observation and an <code>info</code> dictionary.</p> </li> </ul> </li> <li><p><code>render</code></p> <ul> <li><p>Signature:</p> <pre>def render(self) -&gt; RenderFrame | list[RenderFrame] | None\n</pre> </li> <li><p>Produces a visual or textual representation of the current state, depending on the <code>render_mode</code> selected when the environment was created. Returns a frame, a list of frames, or <code>None</code>.</p> </li> </ul> </li> <li><p><code>close</code></p> <ul> <li><p>Signature:</p> <pre>def close(self) -&gt; None\n</pre> </li> <li><p>Frees external resources (windows, simulators, files, sockets) and can be called multiple times safely.</p> </li> </ul> </li> </ul> <p>Other than this, the class specified must be:</p> <ul> <li>The class should expose a top-level attribute called metadata, something along the lines of:<ul> <li><code>    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}</code></li> <li>All render modes should be specified and the rendering frames per second.</li> </ul> </li> <li>The class should have two properties, <code>self.action_space</code> and <code>self.observation_space</code> that states what type of space is used for each (e.g. Discrete, Box, MultiDiscrete) and what size/dimensions these spaces are (this must be known at initialisation time of the class).</li> </ul> <p>Optionally, for usage of environment creation by name within <code>make_env()</code>, you should use the <code>register()</code> function, an example is shown later in this notebook</p>"},{"location":"4-Creating-Envs-News-Vendor/#implementation","title":"Implementation\u00b6","text":"<p>Here, the implementation is left as an exercise for you to complete out some of the stubbed out code (marked as <code>@Solve</code>):</p>"},{"location":"4-Creating-Envs-News-Vendor/#conclusion","title":"Conclusion\u00b6","text":"<p>With this notebook, we have discovered on how you can model any combinatorial problem as a Reinforcement Learning environment using the Gymnasium API. The Gymnasium API is the most popular API for modelling RL environments, aside from multi-agent RL, which typically uses PettingZoo (which is just an extension of Gymnasium to handle both parallel and turn-based multi-agent RL).</p> <p>Notice how the environment we implemented here uses a <code>Box()</code> space, so is not directly trainable with our Tabular Q-Learning implementation, but function approximation based extensions of Tabular Q-Learning will allow compatibility. If you are able to implement the visualisation extension, that is more than sufficient. You can implement a stronger algorithm to support this environment (which also serves as your motivation!) after going through the conclusion notebook.</p> <p>The next notebook will conclude this exemplar, however, the conclusion provides a quite comprehensive yet accessible literature review of the state-of-the-art for solving combinatorial problems using RL. Therefore, it is definitely worth reading!</p>"},{"location":"4-Creating-Envs-News-Vendor/#extensions","title":"Extensions\u00b6","text":""},{"location":"4-Creating-Envs-News-Vendor/#extension-1-implementing-a-renderer","title":"Extension 1. Implementing a Renderer\u00b6","text":"<p>At the moment, while we can interface with the environment through code, there's no way to interactively play the environment as an user. This is helpful for both visualisation and debugging, to make sure your environment is working as intended.</p> <p>Try implementing \"ANSI\" rendering (console text) and write some code that allows you to interactively provide input (in terms of action) at each step. For how rendering works, you may find it useful to refer to the Monty Hall environment at <code>src/environments/monty_hall</code> and consult the Gymnasium documentation at the following URLs:</p> <ul> <li>https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/</li> <li>https://gymnasium.farama.org/api/env/#gymnasium.Env.render</li> </ul>"},{"location":"4-Creating-Envs-News-Vendor/#extension-2-action-masking","title":"Extension 2. Action Masking\u00b6","text":"<ol> <li>Explore the code for the Monty Hall environment provided in the repository at <code>src/environments/monty_hall</code>.</li> <li>Explain how Action Masking is implemented there and used in Tabular Q Learning at <code>src/rl/tabular_q_learning.py</code>.</li> </ol>"},{"location":"4-Creating-Envs-News-Vendor/#extension-3-recording-your-agent","title":"Extension 3. Recording your Agent\u00b6","text":"<p>In Notebook 3, we learnt that during training, we can visualise the cumulative returns of our agent to monitor how well they are performing, since a higher reward equates to better performance.</p> <p>Another common method, and a nice visualisation tool, is viewing recordings of how your agent is performing. This requires sometimes additional processing, but you do not have to record your environment at every time step.</p> <p>For this extension, copy your code from Notebook 3 on the Monty Hall environment, instead of using this particular environment (as Tabular Q-Learning is not applicable here). You can proceed by reading this documentation on the Gymnasium website.</p>"},{"location":"4-Creating-Envs-News-Vendor/#extension-4-make-the-environment-combinatorial","title":"Extension 4. Make the Environment Combinatorial\u00b6","text":"<p>Try adding an additional product to the environment, thus making this environment combinatorial.</p>"},{"location":"5-Conclusion/","title":"5. Conclusion","text":"Illustration of Hierarchical Reinforcement Learning (HRL), as shown in (Bougie and Ichise, 2021)"},{"location":"5-Conclusion/#5-conclusion","title":"5. Conclusion\u00b6","text":"<p>Congratulations for having completed this course. As a reminder, we have covered the following:</p> <ul> <li>How Reinforcement Learning and sequential combinatorial problems work at a high level (notebook 1)</li> <li>Implementation of a foundational Reinforcement Learning algorithm, Tabular Q-Learning, from scratch</li> <li>Visualising training results and performing automated hyperparameter tuning</li> <li>Modelling your own custom environment using the Gymnasium API, allowing you to define any problem space you are working with in a manner that any RL algorithm can solve</li> </ul> <p>Before concluding, there are a few things worth mentioning, specifically in terms of how you can continue your learning beyond what has been covered in this introductory course, and a few things that we can expand on now that you have gained all prerequisite foundational knowledge. Resources and concepts mentioned here are largely provided for reference rather than expecting for you to quickly attain knowledge of all these concepts at once.</p>"},{"location":"5-Conclusion/#reinforcement-learning-algorithms-beyond-the-tabular-setting","title":"Reinforcement Learning Algorithms Beyond the Tabular Setting\u00b6","text":"<p>In this course, we only implemented Tabular Q-Learning, which is a tabular algorithm.</p> <p>There are lots of different settings in Reinforcement Learning, that demand using different approaches:</p> <ul> <li>Online / Offline<ul> <li>Offline RL learns solely from a fixed data set of transitions and rewards \u2014 for example, production logs or expert demonstrations,because online exploration is impossible, unsafe, or too costly</li> <li>Online RL learns in the real environment or a simulation, but as aforementioned, it's not always feasible to create a faithful representation of a real environment as a simulation (e.g. where complex physical laws need to be captured)</li> <li>Offline Reinforcement Learning is a vastly different setting, since exploration isn't possible, and resolves to dealing with counterfactual reasoning problems (\"what if action Y instead of X\" was taken here). It is easily susceptible to out-of-distribution and extrapolation errors. See Liu et al, 2023 and Kumar et al, 2020 for further reading.</li> </ul> </li> <li>Continuous / Discrete<ul> <li>Both an action space and state space can either be discrete or continuous</li> <li>Discrete means the set of possible states or actions can be counted exactly (e.g. the three doors to choose between in Monty Hall)</li> <li>Continuous spaces contain infinitely many values\u2014representing an action might require specifying a real\u2011valued vector such as wheel velocities, steering angles, or joint torques.</li> <li>Our Tabular Q-Learning implementation only handles discrete action spaces and discrete state spaces.</li> <li>As an extension, you may want to consider implementing Deep\u202fQ\u2011Network (DQN) for high\u2011dimensional visual observations (Mnih\u202fet\u202fal.,\u202f2013), while Deep Deterministic Policy Gradient (DDPG) and its successors handle continuous actions (Lillicrap\u202fet\u202fal.,\u202f2015).</li> </ul> </li> <li>Single-agent / Multi-agent<ul> <li>In multi\u2011agent RL several agents learn simultaneously. Their objectives may be cooperative, competitive, or mixed\u2011sum. Training can be centralised (a single learner observes the joint state) or decentralised, and explicit inter\u2011agent communication may be permitted.</li> <li>For example, consider warehouse robot fleets, which is an example of a multi-agent combinatorial sequential decision problem.</li> </ul> </li> <li>Finite time-horizon (Episodic RL) / Infinite time horizon<ul> <li>Episodic environments terminate after a finite number of steps, while continuous (infinite-horizon) tasks run indefinitely.</li> <li>Regardless, continuous  tasks are typically trained by either bootstrapping with a discount factor $\\gamma$\u00a0&lt;\u00a01 or by truncating rollouts to a manageable length so that the (infinite) return remains finite (Sutton\u202f&amp; Barto,\u202f2018).</li> </ul> </li> </ul> <p>Generally, Reinforcement Learning methods can be split into the following:</p> <ul> <li>On-policy / Off-policy<ul> <li>On-policy (e.g. SARSA, PPO) learns only from trajectories sampled from the current policy, whereas off-policy (e.g. Q-learning, DDPG, SAC) can reuse past or externally generated data.</li> <li>On-policy RL is considered more stable, whereas off-policy RL has the potential to be more sample efficient (requiring less experiences to learn from).</li> </ul> </li> <li>Value-based / Policy-based<ul> <li>Value\u2011based methods involve the agent first learning a value function \u2014 a prediction of the long\u2011term reward for each action in every state. When it has to act, it usually chooses the action with the highest predicted value (adding a little randomness during learning, e.g. $\\varepsilon$\u2011greedy). Classic algorithms include: Q\u2011Learning, Deep\u202fQ\u2011Network\u202f(DQN).</li> <li>Policy\u2011based methods have the agent skipping learning value function and learns the policy itself instead \u2014 a set of probabilities for choosing each action. Those probabilities are adjusted by policy\u2011gradient methods: actions that paid off become more likely. This adjustment is simply gradient ascent on the expected reward (think of repeatedly nudging the policy parameters in the direction that raises the score). Popular algorithms include PPO, A3C and REINFORCE.<ul> <li>Since in Notebook 2, we implemented Tabular Q-Learning which is value-based, you might want to consider implementing REINFORCE as a gentler practical introduction to policy-based RL. For further details, see Williams, 1992 and page 328 of Sutton &amp; Barto, 2018.</li> </ul> </li> <li>Actor\u2013critic methods are hybrids which combine both: a policy (actor) chooses actions while a value function (critic) supplies low\u2011variance learning signals.</li> </ul> </li> <li>Model-based / Model-free<ul> <li>Model\u2011free RL (e.g., Tabular\u202fQ\u2011Learning, PPO, SAC) directly optimises a policy or value function from experience, without learning an explicit dynamics model. \u2013 Model\u2011based RL learns (or is provided) a predictive model of the environment\u2019s dynamics and reward, enabling planning for multiple timesteps ahead with imagined rollouts. Notable examples include MuZero (Schrittwieser\u202fet\u202fal.,\u202f2020) and DreamerV3 (Hafner\u202fet\u202fal.,\u202f2023), which achieve strong sample efficiency on many standard benchmarks.</li> </ul> </li> </ul>"},{"location":"5-Conclusion/#high-level-overview-of-rl-algorithms","title":"High-Level Overview of RL Algorithms\u00b6","text":"<p>The following diagram provides a high-level overview of Reinforcement Learning algorithms, derivative of Chapter 2 of the Spinning Up book by OpenAI. This mindmap is by no means exhaustive, and could be further split into on-policy and off-policy algorithms, which would also make an interesting exercise! Some other algorithms cannot be easily categorised, such as Decision Transformers, and not to mention, offline RL algorithms are not mentioned here, nor imitation learning (learning from expert demonstrations).</p>"},{"location":"5-Conclusion/#scaling-to-large-high-dimensional-state-and-action-spaces","title":"Scaling to Large, High-Dimensional State and Action Spaces\u00b6","text":"<p>Combinatorial problems involve an exponential blow-up in terms of the number of states and actions that can be considered. In general, navigating this is an open research problem, but there are still solutions that can be applied in practice, which will be detailed below.</p>"},{"location":"5-Conclusion/#abstraction","title":"Abstraction\u00b6","text":""},{"location":"5-Conclusion/#state-space","title":"State Space\u00b6","text":"<p>Sometimes, states of a MDP are equivalent, such that symmetry reduction techniques can be applied. For example, consider the problem of load balancing (distributing compute workloads between servers) might involve exactly two identical jobs queued with two identical servers, which is reducible to 1 state instead of 2 \u2014 since the two states are said to be indistinguishable. Also, it is possible to directly reduce the size of the state space, e.g. through a technique known as Bisimulation Quotienting (Drakulic et al, 2023).</p> <p>Relational Reinforcement Learning (RRL) pushes symmetry reasoning into first\u2011order logic: instead of enumerating every concrete state, the agent thinks in predicates such as <code>On(block, support)</code> or <code>At(truck, depot)</code> and learns policies parameterised by object variables.  Lifted Q\u2011Learning, graph\u2011attention networks over predicate graphs, and other neuro\u2011symbolic hybrids let one compact policy scale from a two\u2011truck routing toy to a city\u2011wide fleet, so long as the relational schema is unchanged.  The catch is that those elegant, lifted rules still have to be grounded against the real objects at decision time, so action selection can balloon to exponential-time matching problems, and small errors in predicate recognition can poison many instantiations at once.  Moreover, crafting or learning the right predicate vocabulary remains tricky: if key relations are missing or overly abstract, the promised generalisation evaporates, leaving you with the cost of symbolic reasoning but little of the payoff.</p> <p>State abstractions can be learnt, such as through bisimulation metrics or causality-based methods, based on preserving reward relevant distinctions. Training happens in a latent (abstract) space which is much smaller than the real space. For example, see Liu et al., 2024.</p> <p>Graph neural networks (GNNs) provide a compact, permutation\u2011equivariant way to embed MDP states that are most naturally described as graphs \u2014 think vehicle\u2011routing, molecule manipulation or multi\u2011robot coordination. By iteratively aggregating information along edges, a GNN encodes every node into a fixed-size vector whose dimensionality is set by the network architecture, not by the number of nodes in the problem instance. This yields an instance\u2011size\u2013independent state representation that emphasises the relational structure most relevant for decision\u2011making Darvariu\u202fet\u202fal.,\u202f2024.</p>"},{"location":"5-Conclusion/#action-space","title":"Action Space\u00b6","text":"<p>For example, large discrete action spaces can be handled by Stochastic Q-Learning (Fourati et. al., 2024, avoids scanning the entire action set at every step and instead draws upon a smaller random handful of actions each step, picking the best one from that subset. Despite making such a change, convergence guarantees are upheld, because a new subset is drawn every step. This immensely contributes to tractability.</p> <p>Additionally, a multi-dimensional action can be factorised into components, instead of considering each joint action. A shared state encoder is followed by multiple action branches, one for each action dimension. This approach is named as a Branching Dueling Q-Network (BDQ), proposed by Tavakoli et al. (2018).</p> <p>Embeddings of actions can also be considered for actions that have some structure or can be described by features to generalize value estimates across similar actions, for example, see Dulac-Arnold et al. (2015).</p> <p>Another option is considering options, which are macro-actions. These are temporally-extended actions, where sequences of actions can be considered rather than individual actions, which reduces the number of actions that has to be considered. Some emerging methods discover these options automatically, for example see Bacon et al., 2016.</p>"},{"location":"5-Conclusion/#hierarchy-and-decomposition","title":"Hierarchy and Decomposition\u00b6","text":"<p>Let a coarse policy break the exponentially growing action/state space into bite-sized sub-goals, and let specialised policies master those.</p> <p>Hierarchical Reinforcement Learning (HRL) spreads work across several policies, and works as follows:</p> <ol> <li>A high\u2011level meta\u2011controller operates on an abstract, slow\u2011moving representation and periodically outputs a sub\u2011goal $g$ instead of a primitive action.</li> <li>A low\u2011level specialist is conditioned on $(s, g)$ and need only optimise behaviour inside that much smaller goal\u2011centred space.</li> </ol> <p>This \"divide-and-conquer\" reduces sample complexity, makes policies reusable, and lets you mix different learning schemes (model-free, model-based, heuristic) under one roof. This actually considered a form of decision abstraction.</p> <p>However, it requires that your problem has tasks that are decomposable, but if it can, training sub-policies that deal with specific goals is far simpler to do. For multi-task generalisation more broadly, even if the task is not decomposable, you may want to consider Goal-Conditioned Reinforcement Learning (GCRL), see Liu et al, 2022.</p> <p>An example this could be applied to:</p> <p>A fleet of $K$ vehicles must serve $N$ customers that each have a service time\u2011window $[e_i,\\,l_i]$. The environment is sequential: at every decision step a vehicle chooses its next customer (or to return to the depot), travel times may change with real\u2011time traffic, and new requests can pop up while routes are already under way.</p> HRL Level Typical Sub\u2011Goal High\u2011level meta\u2011controller(slow clock, abstract state) \u2022 Decide which cluster of customers a given vehicle should cover next\u2022 Decide when a vehicle should return to depot for loading/charging Low\u2011level specialist(fast clock, concrete state) \u2022 Select the exact next customer inside the chosen cluster\u2022 Execute detailed manoeuvres while respecting time windows and traffic <p>\u201cA meta\u2011controller plus one worker\u201d is merely the simplest instantiation of hierarchical RL. In practice you can vary: (a) how many levels exist and (b) how many policies live at each level to match the combinatorial structure of your problem. For example, see the following figure:</p>"},{"location":"5-Conclusion/#effective-exploration","title":"Effective exploration\u00b6","text":"<p>Despite how large the resultant action or state space ends up, it still should be as effectively explored as possible. Even small discrete spaces benefit from going beyond $\\varepsilon$-greedy exploration, so there's no reason not to consider more effective exploration strategies. For example, Boltzmann exploration (see the extension activity of Notebook 2) provides graded preferences based on Q-values but is still reliant on a temperature schedule.</p> <p>Beyond $\\varepsilon$\u2011greedy and Boltzmann, you can let uncertainty itself guide exploration. Two classic examples are Upper\u202fConfidence\u202fBound\u202f(UCB), which adds an \"optimism bonus\" to actions that have been tried less often, and Thompson\u202fsampling usually draws a fresh policy from an ensemble of value estimates so infrequently chosen moves get more chances.</p> <p>You may want to consult some academic surveys for the latest methods in this direction, for example Amin et. al., 2021 and Hao et. al., 2021.</p>"},{"location":"5-Conclusion/#sparse-rewards","title":"Sparse Rewards\u00b6","text":"<p>Many real-life problems are not able to assign any reward at each time step. In the worst case, a reward can only be assigned at the end of the episode. In Notebook 2, where we implemented Tabular Q-Learning, you might have noted that the Temporal Difference (TD) update is based on looking at one immediate step-ahead. This is clearly quite problematic, however, Q-learning can be adapted to have a $n$-step TD learning algorithm.</p> <p>Beyond this, there are other ways to deal with sparse rewards. One key method is known as reward-shaping, where a heuristic is provided (or perhaps even learned) and combined with the real reward given at each time step. Specifically, potential-based reward shaping, where a heuristic function is defined for each state retains all convergence guarantees. Potential-based reward shaping can formally be defined as $r' = r +\\gamma \\varphi(s') - \\varphi(s)$ where $\\varphi$ is the potential function.</p> <p>There is also some synergy with hierarchical RL which was mentioned earlier, since for sub\u2011policies\u202f/\u202fsub\u2011goals, it can often be easier to define a reward function.</p> <p>Other approaches include intrinsic motivation (e.g. counting state novelty) to drive exploration; Hindsight Experience Replay, which relabels failed episodes with the goals that were actually achieved so that even \"unsuccessful\" rollouts provide positive learning signals; and Align\u2011RUDDER, which uses sequence alignment and contribution analysis to redistribute a delayed terminal reward back to the key earlier actions, effectively turning a sparse reward into a dense one.There are numerous academic surveys on the topic of sparse rewards, and it remains a dominant research area for RL.</p>"},{"location":"5-Conclusion/#implementation-details-of-algorithms","title":"Implementation Details of Algorithms\u00b6","text":"<p>Each algorithm is based on a theoretical foundation, but there are quite a few implementation details you will come across. For example, one of the state-of-the-art policy gradients, named Proximal Policy Optimization (PPO), has 37 implementation details documented here.</p> <p>A great reference point of implementations is CleanRL. Specifically, their implementations are one-file based and comment key details.</p> <p>It's worth noting that testing your implementations in Machine Learning (ML) broadly is quite difficult. That's because unlike traditional software development, bugs do not cause immediately obvious erroneous behaviour such as wrongly processed input or a software crash. Machine Learning can easily fail silently, not working in the way you intended, perhaps even leaking data. However, if you can seed two implementations of algorithms, you should get very similar performance with the same hyperparameters and training length, aside from minor optimizations/differences.</p>"},{"location":"5-Conclusion/#optimality-guarantees-using-reinforcement-learning","title":"Optimality Guarantees using Reinforcement Learning\u00b6","text":"<p>With combinatorial problems, there is the notion of optimality: a solution is considered the best (e.g. lowest cost or shortest time) among all feasible alternatives. Classical exact solvers such as branch\u2011and\u2011bound or dynamic programming can prove optimality by systematically exploring the search space and emitting certificates.</p> <p>Reinforcement Learning is inherently an approximate approach: agents learn policies that often yield high\u2011quality solutions, but they rarely come with global optimality guarantees. While simple tabular settings admit convergence results, these depend on restrictive assumptions (e.g., a fully explored MDP and infinite interaction time). In contrast, PAC guarantees (Probably Approximately Correct) provide theoretical sample\u2010complexity bounds: with probability at least $1 - \\delta$ (e.g., 95% or 99%), the algorithm will, after a finite number of samples, output a policy whose expected return is within $\\varepsilon$ of optimal. However, this remains an approximate guarantee, although this is often enough for many real-life application domains, which when large enough also become intractable for exact solvers.</p>"},{"location":"5-Conclusion/#hybrid-methods","title":"Hybrid Methods\u00b6","text":"<p>Traditional solvers and techniques, such as Branch-and-Bound, can be combined with RL in a fashion that retains exact optimality (InstaDeep, 2023), by prioritising search order using RL (as RL automatically learns heuristics).</p> <p>Therefore, RL can be used to learn intelligent heuristics that make exact algorithms faster or more scalable, while the backbone algorithm provides rigorous guarantees.</p>"},{"location":"5-Conclusion/#verifying-solutions-and-certificates","title":"Verifying Solutions and Certificates\u00b6","text":"<p>An exact solver can also use a solution generated quickly from RL and try to improve it, and if it cannot, this verifies that the solution is optimal. This also provides a strong upper or lower bound  (depending if the objective is minimisation or maximisation) early in the solver's usage. Other RL techniques can still be used in the  combined with techniques that guide the search such as constraint propagation in constraint programming. This provides an ad-hoc verifiably optimal solution.</p> <p>A more advanced notion is for the RL algorithm itself to produce a certificate of quality alongside its solution. For instance, an RL agent might simultaneously learn a dual bound \u2014 that is, a rigorously derived lower bound on the optimal value in a minimisation problem (and vice versa for a maximization problem). Comparing the solution\u2019s cost to this bound tells you how far, at most, the solution could be from optimal.</p>"},{"location":"5-Conclusion/#further-resources","title":"Further Resources\u00b6","text":"<p>You can view some further resources as follows:</p> <ul> <li>For building your Reinforcement Learning knowledge:<ul> <li>Mastering Reinforcement Learning, which is a book accompanied by videos, providing an excellent overview of the various Reinforcement Learning methods out there</li> <li>Reinforcement Learning: An Introduction, a seminal book with its latest edition published in 2018, by Richard S. Sutton and Andrew G. Barto. This book is considered foundational, and both authors heavily contributed to Reinforcement Learning research and helped start the field. However, this book is more on the theoretical side.</li> <li>Spinning Up in Deep RL by OpenAI, which provides a great overview of the state-of-the-art methods (e.g. PPO and actor-critic methods), particularly with deep reinforcement learning.<ul> <li>If you are not familiar with Deep Learning, consider looking at:<ul> <li>Dive into Deep Learning, free online book, with code accompanying each section</li> <li>fast.ai courses, covering advanced deep learning methods from the foundations accompanied by practical implementations</li> </ul> </li> </ul> </li> </ul> </li> <li>Additional combinatorial environments are available at:<ul> <li>Jumanji</li> <li>or-gym, or stands for Operations Research</li> </ul> </li> <li>Interesting papers:<ul> <li>Pointer Networks, used by some methods such as AlphaStar</li> <li>Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective</li> </ul> </li> </ul>"}]}