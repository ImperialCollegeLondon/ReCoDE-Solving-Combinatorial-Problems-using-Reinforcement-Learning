
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
        <link rel="next" href="1-Intro-to-RL/">
      
      
      <link rel="icon" href="assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>Solving Combinatorial Problems using RL</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#solving-combinatorial-problems-using-reinforcement-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="https://imperialcollegelondon.github.io/ReCoDE-home/" title="Solving Combinatorial Problems using RL" class="md-header__button md-logo" aria-label="Solving Combinatorial Problems using RL" data-md-component="logo">
      
  <img src="assets/iclogo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Solving Combinatorial Problems using RL
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Home
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/ImperialCollegeLondon/ReCoDE-Solving-Combinatorial-Problems-using-Reinforcement-Learning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://imperialcollegelondon.github.io/ReCoDE-home/" title="Solving Combinatorial Problems using RL" class="md-nav__button md-logo" aria-label="Solving Combinatorial Problems using RL" data-md-component="logo">
      
  <img src="assets/iclogo.png" alt="logo">

    </a>
    Solving Combinatorial Problems using RL
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ImperialCollegeLondon/ReCoDE-Solving-Combinatorial-Problems-using-Reinforcement-Learning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-outcomes" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Outcomes 🎓
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#target-audience" class="md-nav__link">
    <span class="md-ellipsis">
      Target Audience 🎯
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites ✅
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Prerequisites ✅">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#academic" class="md-nav__link">
    <span class="md-ellipsis">
      Academic 📚
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#system" class="md-nav__link">
    <span class="md-ellipsis">
      System 💻
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#getting-started" class="md-nav__link">
    <span class="md-ellipsis">
      Getting Started 🚀
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#disciplinary-background" class="md-nav__link">
    <span class="md-ellipsis">
      Disciplinary Background 🔬
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#software-tools" class="md-nav__link">
    <span class="md-ellipsis">
      Software Tools 🛠️
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Project Structure 🗂️
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practice-notes" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practice Notes 📝
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#estimated-time" class="md-nav__link">
    <span class="md-ellipsis">
      Estimated Time ⏳
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Additional Resources 🔗
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#licence" class="md-nav__link">
    <span class="md-ellipsis">
      Licence 📄
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="1-Intro-to-RL/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="2-Tabular-Q-Learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Tabular Q Learning
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="3-Experiments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Experiments
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="4-Creating-Envs-News-Vendor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Creating your own Environment: News Vendor
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="5-Conclusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Conclusion
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learning-outcomes" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Outcomes 🎓
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#target-audience" class="md-nav__link">
    <span class="md-ellipsis">
      Target Audience 🎯
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites ✅
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Prerequisites ✅">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#academic" class="md-nav__link">
    <span class="md-ellipsis">
      Academic 📚
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#system" class="md-nav__link">
    <span class="md-ellipsis">
      System 💻
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#getting-started" class="md-nav__link">
    <span class="md-ellipsis">
      Getting Started 🚀
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#disciplinary-background" class="md-nav__link">
    <span class="md-ellipsis">
      Disciplinary Background 🔬
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#software-tools" class="md-nav__link">
    <span class="md-ellipsis">
      Software Tools 🛠️
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Project Structure 🗂️
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practice-notes" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practice Notes 📝
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#estimated-time" class="md-nav__link">
    <span class="md-ellipsis">
      Estimated Time ⏳
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Additional Resources 🔗
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#licence" class="md-nav__link">
    <span class="md-ellipsis">
      Licence 📄
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<!--
This includes your top-level README as you index page i.e. homepage.

This will not be the best approach for all exemplars, so feel free to customise
your index page as you see fit.
-->

<h1 id="solving-combinatorial-problems-using-reinforcement-learning">Solving Combinatorial Problems using Reinforcement Learning</h1>
<p>Combinatorial optimization problems are frequently characterized by being intractable for classical algorithms or prone to suboptimal solutions. Such problems naturally arise in logistics (e.g., vehicle routing), scheduling (e.g., job shop), network design (e.g., flow), finance (e.g., portfolio selection), and beyond. Even minor improvements in these domains can yield substantial benefits in cost, efficiency, or strategy. However, designing heuristics and metaheuristics to tackle their complexity is time-consuming.</p>
<p><strong>Reinforcement Learning</strong> (RL) has excelled at sequential decision-making tasks in fields ranging from autonomous driving and industrial control to robotics, protein folding, theorem proving, and multiagent games such as chess and go, where it has achieved superhuman performance.</p>
<p><img
  src="assets/README.png"
  alt="README Image"
  style="float: right; width: 35%; height: auto; margin: 0 0 1em 1em;"
/>  </p>
<p>In this exemplar, we will focus on learning to use Reinforcement Learning for solving sequential combinatorial problems, where an optimal strategy involves taking specific actions in a sequence while also responding to a probabilistic setting (environment). Notably, Reinforcement Learning is able to learn the state and action space, so it is able to effectively search these spaces for optimal solutions as opposed to exhaustive searches in classical algorithms, without any heuristics that require expert knowledge to correctly derive.</p>
<p>We will start by implementing a foundational algorithm, Tabular Q Learning, then learn how to apply it in a pre-supplied environment, involving the famous <strong>Monty Hall</strong> problem, where we will also explore hyperparameter tuning and visualisation of training. After this, we will learn how you can apply RL to any problem space of interest by creating your own environment, where we will walk through an example implementing an environment from scratch for the seminal News Vendor problem from inventory management.</p>
<!-- ![Scikit Camera Image](assets/readme-img.png) -->

<!-- Author information -->
<p>This exemplar was developed at Imperial College London by Omar Adalat in
collaboration with Dr. Diego Alonso Alvarez from Research Software Engineering and
Dr. Jesús Urtasun Elizari from Research Computing &amp; Data Science at the Early Career
Researcher Institute.</p>
<h2 id="learning-outcomes">Learning Outcomes 🎓</h2>
<p>After completing this exemplar, students will:</p>
<ul>
<li>Explain the core principles of Reinforcement Learning, and be able to identify when and where it is applicable to a problem space, with a particular focus on combinatorial problems for this project.</li>
<li>Develop an implementation of a foundational algorithm, Tabular Q Learning, starting from basic principles and concepts.</li>
<li>Gain the ability to perform experimental validation of the trained Reinforcement Learning algorithm, visualising the learning over training episodes.</li>
<li>Design hyperparameter tuning configurations that can automatically be applied to retrieve the optimal set of hyperparameters.</li>
<li>Generalise to non-supplied environments by learning how to create your own Reinforcement Learning environment, allowing you to apply Reinforcement Learning to any problem space that you are interested in.</li>
</ul>
<!-- Audience. Think broadly as to who will benefit. -->
<h2 id="target-audience">Target Audience 🎯</h2>
<p>This exemplar is broadly applicable to anyone interested in solving sequential decision problems, which comes up ubiquitously across many domains and industries (e.g. protein synthesis, self-driving cars, planning &amp; scheduling, and strategic games). Specifically, although we focus on sequential combinatorial problems which are a specific flavour of sequential decision problems, the underlying concepts are the same between both.</p>
<p>Our exemplar suitable for students, researchers and engineers alike, and academic prerequisite knowledge is not assumed, aside from some confidence in using Python.</p>
<!-- Requirements.
What skills and knowledge will students need before starting?
e.g. ECRI courses, knowledge of a programming language or library...

Is it a prerequisite skill or learning outcome?
e.g. If your project uses a niche library, you could either set it as a
requirement or make it a learning outcome above. If a learning outcome,
you must include a relevant section that helps with learning this library.
-->
<h2 id="prerequisites">Prerequisites ✅</h2>
<h3 id="academic">Academic 📚</h3>
<ul>
<li>Basic familiarity with Python &amp; basic programming skills is required to solve the exercises notebooks</li>
<li>Some math background in the basics of set theory and probability theory is helpful but not required</li>
</ul>
<h3 id="system">System 💻</h3>
<ul>
<li><a href="https://docs.astral.sh/uv/">Astral's uv</a> Python package and project manager</li>
<li>An integrated development environment (IDE) for developing and running Python, <a href="https://code.visualstudio.com/">Visual Studio Code (VS Code)</a> with Python &amp; Jupyter Notebook extensions is the easiest to set up and use. VS Code should automatically prompt you to install the required extensions that you need, but you can refer to here for the <a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python">Python extension</a> and the <a href="https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter">Jupyter</a> extension is available for notebook support</li>
<li>10 GB of disk space</li>
</ul>
<h2 id="getting-started">Getting Started 🚀</h2>
<ol>
<li>Start by cloning the repository, either using the GitHub interface or Git directly (<code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-Solving-Combinatorial-Problems-using-Reinforcement-Learning</code>).</li>
<li>Install Astral's <code>uv</code> if not already installed from the following URL: <a href="https://docs.astral.sh/uv/">https://docs.astral.sh/uv/</a></li>
<li>Run the command <code>uv sync</code> in the cloned repository directory. This will install the correct version of Python (scoped to the directory under <code>uv</code>) and gather all dependencies needed.</li>
<li>Create a virtual environment under which the Jupyter Notebooks will run under, which will be scoped to the project directory. Simply run <code>uv venv --python 3.12</code>. When running any notebook, use the virtual environment created for Python 3.12 in the current directory's path, VS Code will give you a list selection of virtual environments to run under (you can also switch this in the top right of a notebook as of the time of writing).</li>
<li>Ensure that your notebook's working directory is set to the root of the project directory (the top level folder that lists <code>notebooks</code>, <code>README.md</code>, <code>src</code>, etc). If you are using VS Code, this is already specified by <code>.vscode/settings.json</code> of this repository. If you are using a different IDE, be sure to specify this equivalently, it will be evident that this is working if notebooks are able to properly import all local imports.</li>
<li>Navigate to the five notebooks in the directory <code>/notebooks/</code> and complete them in order, running the exercises which will be checked against automated tests and checking the solutions if at any time you are stuck!</li>
</ol>
<h2 id="disciplinary-background">Disciplinary Background 🔬</h2>
<p>Reinforcement Learning is a powerful learning paradigm in Artificial Intelligence &amp; Computer Science. While Deep Learning and general Machine Learning are very interesting, often the focus is on making a single isolated decision as in the tasks of classification or regression. Reinforcement Learning, which at the state-of-the-art level also uses Deep Learning for effective learning, is important to learn and master for solving more complex tasks that involve sequential decisions.</p>
<p>Specifically, as it solves sequential decision problems, it is incredibly useful in an interdisciplinary manner for various problems that arise such as the aforementioned: scheduling, protein synthesis, finance, autonomous vehicles and beyond.</p>
<!-- Software. What languages, libraries, software you use. -->
<h2 id="software-tools">Software Tools 🛠️</h2>
<ul>
<li>Python, with version and dependencies managed by Astral's uv</li>
<li>Weights &amp; Biases, a web platform for experiment tracking best practices and hyperparameter tuning</li>
</ul>
<p>A selection of key Python libraries/packages used are listed below:
- Gymnasium, allowing to define custom RL environments which conform to a standard interface
- Pygame, for visualisation of the environments
- Matplotlib, for visualisation of training results by plotting charts and diagrams
- Jupyter Notebooks, for literate programming &amp; interactive content</p>
<!-- Repository structure. Explain how your code is structured. -->
<h2 id="project-structure">Project Structure 🗂️</h2>
<p>Overview of code organisation and structure.</p>
<div class="highlight"><pre><span></span><code>.
├── notebooks
│ ├── 1-Intro-to-RL.ipynb
│ ├── 2-Tabular-Q-Learning.ipynb
│ ├── 3-Experiments.ipynb
│ ├── 4-Custom-Envs-News-Vendor.ipynb
│ ├── 5-Conclusion.ipynb
├── src
│ ├── environments
│ ├───── monty_hall
│       │          └── env.py
│       │          └── state.py
│       │          └── renderer.py
│       │          └── discrete_wrapper.py
│ ├───── news_vendor
│       │          └── env.py
│       │          └── state.py
│       │          └── renderer.py
│       │          └── discrete_wrapper.py
│ ├── rl
│   │   └── common.py
│   │   └── tabular_q_learning.py
├── docs
└── test
</code></pre></div>
<p>Code is organised into logical components:</p>
<ul>
<li><code>notebooks</code> for tutorials and exercises<ul>
<li><code>solutions</code> contains full solutions to all exercises, implementing all incomplete functions</li>
<li><code>extras</code> contains notebooks that allow you to interactively visualise the <em>Monty Hall</em> and <em>News Vendor</em> environments</li>
</ul>
</li>
<li><code>src</code> for core code<ul>
<li><code>monty_hall</code> provides the full implementation of the Monty Hall Gymnasium environment. This is something you are expected to import in for Notebook 3. However, later on you can explore this directory in terms of how everything is implemented, for example the discrete state space wrapper, action masking, and visualisation. It may be useful as a reference for any environments you create in the future!</li>
<li><code>news_vendor</code> is a full reference/target implementation for Notebook 4.</li>
<li><code>rl</code> is a reference implementation for Notebook 2, particularly focused on Tabular Q Learning.</li>
</ul>
</li>
<li><code>docs</code> for documentation</li>
<li><code>test</code> for testing scripts</li>
</ul>
<!-- Best practice notes. -->
<h2 id="best-practice-notes">Best Practice Notes 📝</h2>
<ul>
<li>Package (dependency) management and Python version management is provided by <code>uv</code>, which allows a perfectly replicable development environment</li>
<li>Reference code is entirely documented and commented using <a href="https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html#example-google">Google's Style of Python Docstrings</a></li>
<li>Experiments are stored and tracked using <a href="https://wandb.ai">Weights &amp; Biases</a>, which allows long-term access to results of experiments, accompanied by all necessary information to replicate such experiments such as hyperparameters</li>
</ul>
<h2 id="estimated-time">Estimated Time ⏳</h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Notebook 1. Intro to RL</td>
<td>3 hours</td>
</tr>
<tr>
<td>Notebook 2. Tabular Q Learning</td>
<td>6 hours</td>
</tr>
<tr>
<td>Notebook 3. Experiments</td>
<td>2 hours</td>
</tr>
<tr>
<td>Notebook 4. Custom environment: News Vendor</td>
<td>4 hours</td>
</tr>
<tr>
<td>Notebook 5. Conclusion</td>
<td>3 hours</td>
</tr>
</tbody>
</table>
<p><strong>Total time</strong>: 18 hours</p>
<h2 id="additional-resources">Additional Resources 🔗</h2>
<ul>
<li>For building your Reinforcement Learning knowledge:<ul>
<li><a href="https://gibberblot.github.io/rl-notes/index.html#">Mastering Reinforcement Learning</a>, which is a book accompanied by videos, providing an excellent overview of the various Reinforcement Learning methods out there</li>
<li><a href="http://incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a>, a seminal book with its latest edition published in 2018, by Richard S. Sutton and Andrew G. Barto. This book is considered foundational, and both authors heavily contributed to Reinforcement Learning research and helped start the field. However, this book is more on the theoretical side.</li>
<li><a href="https://spinningup.openai.com/en/latest/">Spinning Up in Deep RL by OpenAI</a>, which provides a great overview of the state-of-the-art methods (e.g. PPO and actor-critic methods), particularly with deep reinforcement learning.<ul>
<li>If you are not familiar with Deep Learning, consider looking at:<ul>
<li><a href="https://d2l.ai/">Dive into Deep Learning</a>, free online book, with code accompanying each section</li>
<li><a href="https://www.fast.ai/">fast.ai courses</a>, covering advanced deep learning methods from the foundations accompanied by practical implementations</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Additional combinatorial environments are available at:<ul>
<li><a href="https://github.com/instadeepai/jumanji">Jumanji</a></li>
<li><a href="https://github.com/hubbs5/or-gym">OR-gym</a>, OR stands for Operations Research </li>
</ul>
</li>
<li>Specifically for attaining better performance in combinatorial RL, you may want to investigate:<ul>
<li>More advanced exploration methods, other than greedy-epsilon, starting with Boltzmann</li>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf">Pointer Networks</a>, used by some methods such as <a href="https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/">AlphaStar</a></li>
<li><a href="https://arxiv.org/abs/2405.10310">Stochastic Q Learning</a> for handling large action spaces</li>
<li>Abstraction methods for lowering the complexity of the state and action space</li>
</ul>
</li>
</ul>
<!-- LICENCE.
Imperial prefers BSD-3. Please update the LICENSE.md file with the current year.
-->
<h2 id="licence">Licence 📄</h2>
<p>This project is licensed under the <a href="https://github.com/ImperialCollegeLondon/ReCoDE-Solving-Combinatorial-Problems-using-Reinforcement-Learning/blob/main/LICENSE.md">BSD-3-Clause license</a>.</p>
<!-- Add more files in the `docs/` directory for them to be automatically
included in the Mkdocs documentation -->












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["navigation.instant", "navigation.top", "toc.follow", "content.code.annotate"], "search": "assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.50899def.min.js"></script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      
    
  </body>
</html>