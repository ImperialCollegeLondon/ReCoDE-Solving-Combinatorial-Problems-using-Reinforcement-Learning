{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69601fa7",
   "metadata": {},
   "source": [
    "# 3. Creating your own Environment: News Vendor\n",
    "\n",
    "In this notebook, we will cover how you can create your own Reinforcement Learning environment that conforms to the [Gymnasium](https://gymnasium.farama.org/) interface/API provided by the Farama Foundation. \n",
    "\n",
    "## Why should we conform to this API?\n",
    "An API (application‑programming interface) is simply a set of function signatures and data conventions: `reset()`, `step()`, `observation_space`, etc. that any compliant environment must expose. Think of it as the rules of the road that let agents and environments plug into each other without surprises.\n",
    "\n",
    "When your environment follows this contract, every agent implementation in the Gymnasium ecosystem (Stable‑Baselines 3, CleanRL, RLlib, Ray Tune, etc.) can train on it instantly without any glue code nor hacky workarounds. Thus, you gain **interoperability.** Moreover, you gain free **additional tooling** by inheriting Gymnasium’s wrappers, vectorized execution helpers, monitoring utilities, and visualizers, saving you from reinventing episode logging, *video capture*, or parallel rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb6eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.internal.tester import make_tester, run_tests\n",
    "from notebooks.internal.exercises_4_news_vendor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d1a68",
   "metadata": {},
   "source": [
    "## Specifying Environments\n",
    "Specifying an environment corresponds to specifying a *Markov Decision Process*, as you might have optionally read within Notebook 2. Without going into all the formal jargon and math, here's what we need to specify:\n",
    "- **State space**: What are the possible states of the system we can be in, reached by taking actions from the initial state?\n",
    "- **Action space**: What actions can be taken by the agent?\n",
    "    - An **action mask** can also optionally be provided if applicable to the environment.\n",
    "- **Transition function**: if probabilistic transitions are involved from taking actions in states, then you must also provide the accompanying transition probabilities. You are in control of how the agent moves through the environment as you are the **environment designer**.\n",
    "- **Reward function**: by convention, the environment provides a reward function that serves as some sort of ground truth. For example, in Monty Hall, if they gain a car we can provide a reward of 100, and if they gain a goat, we could provide a lesser reward of 10.\n",
    "    - Users can override your reward function if they would like to try something else though just by implementing a wrapper: that's just a slice of the expressive power you gain by using Gymnasium!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab0418",
   "metadata": {},
   "source": [
    "## News Vendor\n",
    "Here, we are covering an environment called the **News Vendor** problem. While an implementation of this already exists in [or-gym](https://github.com/hubbs5/or-gym), we will try to implement it from scratch to try to understand the environment creation process from scratch.\n",
    "\n",
    "> The (multi‑period) **News Vendor problem** models a trader who must decide how many perishable units to order *before* seeing demand that is random but statistically known. Ordering too much leaves leftover inventory that incurs holding or disposal costs, while ordering too little produces lost‑sales penalties or forgone margin; the optimal quantity balances these two expected costs at the critical ratio where the probability of selling an extra unit equals the ratio of under‑stocking cost to the sum of under‑ and over‑stocking costs.\n",
    "\n",
    "At the end of each period, stock that was not sold is \"expired\" (removed) and a holding penalty is applied for any excess stock. A goodwill penalty (shortage penalty) is applied if stock is not held despite it being in demand. Demand is probabilistically modelled. The agent observes stochastic price, cost, holding - and shortage‑penalty parameters plus an order‑pipeline vector, chooses a non‑negative order up to capacity each step, and receives profit (sales revenue minus discounted purchase, holding, and shortage costs). \n",
    "\n",
    "Therefore, this problem is a seminal problem that arises in inventory management, in terms of fast-moving consumer goods (FMCGs) and procurement.\n",
    "\n",
    "You are able to play the environment in `notebooks/extras/play_news_vendor.ipynb` if you would like to see how it works before proceeding, as it already comes with batteries attached (visualisation).\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img\n",
    "    src=\"assets/news_vendor.png\"\n",
    "    alt=\"News Vendor Environment\"\n",
    "    style=\"display:block;margin:0 auto;width:50%;\"\n",
    "  >\n",
    "  <figcaption>Figure of the News Vendor Environment visualised</figcaption>\n",
    "</figure>\n",
    "The classic single-product News Vendor problem is not considered a combinatorial-optimization problem; it is a continuous, single-variable stochastic optimization. However, many *extensions* of the problem, particularly those that add integer quantities, multiple items, or other discrete choices do turn it into a combinatorial problem. In this notebook, the focus is on the actual modelling of an environment in terms of the API - you can apply the same principles to combinatorial and non-combinatorial problems, and this is easy enough to model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f1408",
   "metadata": {},
   "source": [
    "## Conforming to Gymnasium's API\n",
    "\n",
    "### Spaces\n",
    "Think of each space as a contract that tells an RL agent (and you!) what valid actions and observations look like.\n",
    "\n",
    "| Space         | What it represents                                                                                                          | Quick constructor                           | Typical use                                                                                   | Tiny demo                                                                                                                             |\n",
    "|---------------|-----------------------------------------------------------------------------------------------------------------------------|---------------------------------------------|------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Box           | An n‑dimensional box of real or integer values (each dimension has its own lower/upper bound, possibly plus/minus infinity) | Box(low, high, shape, dtype)               | Continuous actions (torques, joystick axes) or vector observations (positions, pixel stacks). | `from gymnasium.spaces import Box; obs = Box(-1.0, 1.0, shape=(3,), dtype=float32); print(obs.sample())  # e.g. [-0.12  0.77 -0.55]` |\n",
    "| Discrete      | A finite set of consecutive integers {start, ..., start+n‑1}.                                                               | Discrete(n, start=0)                       | Classic control actions (left/right), indexes into lookup tables.                             | `action = Discrete(3)  # {0,1,2}; print(action.sample()) `                                                                         |\n",
    "| MultiBinary   | A fixed‑shape binary array (0/1).                                                                                            | MultiBinary(n) or MultiBinary((rows, cols)) | Keyboards, on/off sensors, one‑hot vectors.                                                   | `keys = MultiBinary(8); print(keys.sample())`                                                                                     |\n",
    "| MultiDiscrete | Cartesian product of several independent Discrete spaces, each with its own range.                                          | MultiDiscrete([n0, n1, ...]) (or ndarray)  | Game‑controller combos: e.g. arrow‑keys x fire x jump.                                        | `pad = MultiDiscrete([5, 2, 2]); print(pad.sample())  # e.g. [3 0 1]`                                                             |\n",
    "| Text          | Variable‑length strings drawn from a character set.                                                                         | Text(max_len, min_len=1, charset=...)      | Natural‑language prompts, serial numbers, chat messages.                                      | `txt = Text(10); print(txt.sample())`                                                                                             |\n",
    "\n",
    "\n",
    "In this exemplar, we will only consider `Discrete` and `Box` spaces.\n",
    "\n",
    "Every space must have a known-size at initialization, e.g. `Discrete(n)`. For example, in Monty Hall, you can open one of three doors, so there's three actions. After revealing the first door, you can't choose a revealed door, but there are still 3 actions that can be constrained by action masking such to only allow the agent or user to pick only from the set of 2 valid actions. \n",
    "\n",
    "You can view more details in the [Gymnasium documentation](https://gymnasium.farama.org/api/spaces/).\n",
    "\n",
    "### The 'Contract' required\n",
    "\n",
    "Gymnasium also provides a couple of tutorials for creating a simple Gridworld environment [here](https://gymnasium.farama.org/introduction/create_custom_env/) and a more complete tutorial [with rendering](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/).\n",
    "\n",
    "There are actually only four functions required to implement an environment as mandated by Gymnasium:\n",
    "* **`step`**\n",
    "\n",
    "  * Signature:\n",
    "\n",
    "    ```python\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, bool, dict[str, Any]]\n",
    "    ```\n",
    "  * Advances the environment one time‑step and returns the next observation, reward, `terminated`, `truncated`, and an `info` dictionary.\n",
    "\n",
    "* **`reset`**\n",
    "\n",
    "  * Signature:\n",
    "\n",
    "    ```python\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None\n",
    "    ) -> tuple[ObsType, dict[str, Any]]\n",
    "    ```\n",
    "  * Starts a new episode (optionally reseeding RNG and/or passing episode‑level options). Returns the initial observation and an `info` dictionary.\n",
    "\n",
    "* **`render`**\n",
    "\n",
    "  * Signature:\n",
    "\n",
    "    ```python\n",
    "    def render(self) -> RenderFrame | list[RenderFrame] | None\n",
    "    ```\n",
    "  * Produces a visual or textual representation of the current state, depending on the `render_mode` selected when the environment was created. Returns a frame, a list of frames, or `None`.\n",
    "\n",
    "* **`close`**\n",
    "\n",
    "  * Signature:\n",
    "\n",
    "    ```python\n",
    "    def close(self) -> None\n",
    "    ```\n",
    "  * Frees external resources (windows, simulators, files, sockets) and can be called multiple times safely.\n",
    "\n",
    "Other than this, the class specified must be:\n",
    "* The class should expose a top-level attribute called metadata, something along the lines of:\n",
    "    * `    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}`\n",
    "    * All render modes should be specified and the rendering frames per second.\n",
    "* The class should have two properties, `self.action_space` and `self.observation_space` that states what type of space is used for each (e.g. Discrete, Box, MultiDiscrete) and what size/dimensions these spaces are (this must be known at initialisation time of the class).\n",
    "\n",
    "Optionally, for usage of environment creation by name within `make_env()`, you should use the `register()` function, an example is shown later in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266e967",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Here, the implementation is left as an exercise for you to complete out some of the stubbed out code (marked as `@Solve`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da447e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────── #\n",
    "#                                 Configuration                                    #\n",
    "# ──────────────────────────────────────────────────────────────────────────────── #\n",
    "@dataclass\n",
    "class NewsVendorConfig:\n",
    "    # ── Demand / system limits ──\n",
    "    lead_time: int = 5                 # periods before an order arrives\n",
    "    max_inventory: int = 4_000         # cap on pipeline + on‑hand units\n",
    "    max_order_quantity: int = 2_000    # upper bound for a single action\n",
    "\n",
    "    # ── Cost parameters (upper bounds for random sampling) ──\n",
    "    max_sale_price: float = 100.0\n",
    "    max_holding_cost: float = 5.0\n",
    "    max_lost_sales_penalty: float = 10.0\n",
    "    max_demand_mean: float = 200.0              # Poisson mean upper bound\n",
    "\n",
    "    # ── Episode & discount ──\n",
    "    max_steps: int = 40\n",
    "    gamma: float = 1.0                 # discount on purchase cost\n",
    "\n",
    "class NewsVendorEnv(gym.Env):\n",
    "    \"\"\"Gymnasium implementation of the classic multi-period news-vendor problem.\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        config: NewsVendorConfig = NewsVendorConfig(),\n",
    "        render_mode: str | None = None,\n",
    "        seed: int | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\" A Gymnasium interface of the Multi-Period News Vendor with Lead Times from Balaji et. al.\n",
    "\n",
    "        The News Vendor problem is a seminal problem within inventory management, see: \n",
    "            Foundations of Inventory Management (Zipkin, 2000)\n",
    "\n",
    "        Inventory orders are not instantaneous and have multi-period leadtimes. There are costs inccured for holding\n",
    "        unsold inventory, although unsold inventory expires at the end of each period. There are also penalties\n",
    "        associated with losing goodwill by having unsold inventory.\n",
    "\n",
    "        Observation:\n",
    "            Type: Box\n",
    "            State Vector: S = (p, c, h, k, mu, x_l, x_l-1)\n",
    "            p = price\n",
    "            c = cost\n",
    "            h = holding cost\n",
    "            k = lost sales penalty\n",
    "            mu = mean of demand distribution\n",
    "            x_l = order quantities in the queue\n",
    "\n",
    "        Initial state:\n",
    "            Parameters p, c, h, k, and mu, with no inventory in the pipeline.\n",
    "\n",
    "        Actions:\n",
    "            Type: Box\n",
    "            Amount of product to order.\n",
    "\n",
    "        Reward:\n",
    "            Sales minus discounted purchase price, minus holding costs for\n",
    "            unsold product or penalties associated with insufficient inventory.\n",
    "\n",
    "        Episode termination:\n",
    "            By default, the environment terminates within 40 time steps.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.config = config\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # ╭─────────────────── Observation space ───────────────────╮\n",
    "        # state = [p, c, h, k, μ, pipeline_1,  …, pipeline_n]\n",
    "        #                          ↑        ↑\n",
    "        #                          newest   next to arrive\n",
    "        self.obs_dim = self.config.lead_time + 5\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.zeros(self.obs_dim, dtype=np.float32),\n",
    "            high=np.array(\n",
    "                [self.config.max_sale_price,        # p\n",
    "                 self.config.max_sale_price,        # c  (bounded by price)\n",
    "                 self.config.max_holding_cost,      # h\n",
    "                 self.config.max_lost_sales_penalty,# k\n",
    "                 self.config.max_demand_mean]                # μ\n",
    "                + [self.config.max_order_quantity] * self.config.lead_time,\n",
    "                dtype=np.float32,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # ─── Action space ─── #\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.0], dtype=np.float32),\n",
    "            high=np.array([self.config.max_order_quantity], dtype=np.float32),\n",
    "        )\n",
    "\n",
    "        # ─── renderer (optional) ───\n",
    "        self._renderer = None\n",
    "\n",
    "        # ─── Initialisation ─── #\n",
    "        self.state = self.reset(seed=seed)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────────── #\n",
    "    #                                Private helpers                                   #\n",
    "    # ──────────────────────────────────────────────────────────────────────────────── #\n",
    "   \n",
    "    def _sample_economic_parameters(self) -> None:\n",
    "        \"\"\"Draw random price/cost/penalties for a fresh episode.\"\"\"\n",
    "        self.price = max(1.0, self.np_random.random() * self.config.max_sale_price)\n",
    "            \n",
    "        # @Solve: sample ∼ U(0, price) then lower‑bound at 1.0\n",
    "        self.cost = ...   # ← Unit purchase cost ≤ price\n",
    "        \n",
    "        self.holding_cost_rate     = self.np_random.random() * min(self.cost, self.config.max_holding_cost)\n",
    "        \n",
    "        # @Solve: sample ∼ U(0, max_lost_sales_penalty)\n",
    "        self.lost_sales_penalty = ...  # ← goodwill‑loss penalty\n",
    "        \n",
    "        self.mu    = self.np_random.random() * self.config.max_demand_mean # mu = max demand mean\n",
    "    \n",
    "    def _reset_state(self) -> np.ndarray:\n",
    "        \"\"\"Reset economic parameters and clear the pipeline.\"\"\"\n",
    "        self._sample_economic_parameters()\n",
    "        pipeline = np.zeros(self.config.lead_time, dtype=np.float32)\n",
    "        self.current_step = 0\n",
    "        return np.concatenate((\n",
    "            np.array([self.price, self.cost, self.holding_cost_rate, self.lost_sales_penalty, self.mu], dtype=np.float32),\n",
    "            pipeline,\n",
    "        ))\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────────── #\n",
    "    #                                 Gymnasium API                                    #\n",
    "    # ──────────────────────────────────────────────────────────────────────────────── #\n",
    "    def reset(self, *, seed: int | None = None, options=None):\n",
    "        \"\"\"Resets the environment, whenever an episode has terminated or is beginning.\n",
    "\n",
    "        Args:\n",
    "            seed (int or None): reset the environment with a specific seed value. Defaults to None.\n",
    "            options: unused, mandated by the Gymnasium interface. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            State vector\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.state = self._reset_state()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # ── 1: Ensure scalar float ──\n",
    "        if isinstance(action, (np.ndarray, list)):\n",
    "            action = float(np.asarray(action).flatten()[0])\n",
    "        order_qty = np.clip(\n",
    "            action,\n",
    "            0.0,\n",
    "            min(\n",
    "                self.config.max_order_quantity,\n",
    "                self.config.max_inventory - self.state[5:].sum(),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # ── 2: Demand for this period ──\n",
    "        demand = self.np_random.poisson(lam=self.mu)\n",
    "\n",
    "        # ── 3: Inventory available today ──\n",
    "        pipeline = self.state[5:]\n",
    "        inv_on_hand = order_qty if self.config.lead_time == 0 else pipeline[0]\n",
    "\n",
    "        # ── 4: Sales outcomes ──\n",
    "        sales_revenue   = min(inv_on_hand, demand) * self.price\n",
    "        excess_inventory = max(0.0, inv_on_hand - demand)\n",
    "        short_inventory  = max(0.0, demand - inv_on_hand)\n",
    "\n",
    "        # ── 5: Costs ──\n",
    "        purchase_cost      = order_qty * self.cost * (self.config.gamma ** self.config.lead_time)\n",
    "        # @Solve\n",
    "        holding_cost       = ...\n",
    "        # @Solve\n",
    "        lost_sales_penalty = ...\n",
    "\n",
    "        # ── 6: Reward ──\n",
    "        reward = sales_revenue - purchase_cost - holding_cost - lost_sales_penalty\n",
    "        if isinstance(reward, Iterable):\n",
    "            reward = float(np.squeeze(reward))\n",
    "\n",
    "        # ── 7: Advance pipeline ──\n",
    "        new_pipeline = np.zeros(self.config.lead_time, dtype=np.float32)\n",
    "        if self.config.lead_time > 0:\n",
    "            new_pipeline[:-1] = pipeline[1:]\n",
    "            new_pipeline[-1]  = order_qty\n",
    "        self.state = np.hstack([self.state[:5], new_pipeline]).astype(np.float32)\n",
    "\n",
    "        # ── 8: Episode termination ──\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= self.config.max_steps\n",
    "        truncated  = False  # No time‑limit truncation beyond max_steps\n",
    "\n",
    "        return self.state, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode: str | None = None):\n",
    "        if self.render_mode is None:\n",
    "            return None\n",
    "            \n",
    "        if not self._renderer:\n",
    "            raise ValueError(\"render_mode was None when env was created.\")\n",
    "        return self._renderer.render(self.state)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the environment, performing some basic cleanup of resources.\"\"\"\n",
    "        if self._renderer:\n",
    "            self._renderer.close()\n",
    "            self._renderer = None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────── #\n",
    "#               Register so users can call gymnasium.make()                        #\n",
    "# ──────────────────────────────────────────────────────────────────────────────── #\n",
    "register(\n",
    "    id=\"NewsVendor-v0\",\n",
    "    entry_point=\"environments.news_vendor.env:NewsVendorEnv\",\n",
    "    max_episode_steps=NewsVendorConfig().max_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f489532d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14f9ced6b054c80bac7c599b34194b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='success', description='Run Tests', icon='check', style=Butt…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your code here - don't modify this cell!\n",
    "def economic_parameters(seed: int):\n",
    "    \"\"\"Return (p, c, h, k, μ) after env.reset(seed).\"\"\"\n",
    "    env = NewsVendorEnv()\n",
    "    env.reset(seed=seed)\n",
    "    return (env.price,\n",
    "            env.cost,\n",
    "            env.holding_cost_rate,\n",
    "            env.lost_sales_penalty,\n",
    "            env.mu)\n",
    "\n",
    "def first_step_reward(seed: int, action: float):\n",
    "    \"\"\"Reward obtained from the very first step after env.reset(seed).\"\"\"\n",
    "    env = NewsVendorEnv()\n",
    "    env.reset(seed=seed)\n",
    "    _, reward, *_ = env.step(action)\n",
    "    return reward\n",
    "\n",
    "make_tester(economic_parameters, economic_parameter_cases)\n",
    "make_tester(first_step_reward, first_step_reward_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7f171",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "---\n",
    "\n",
    "## Extension 1. Implementing a Renderer\n",
    "At the moment, while we can interface with the environment through code, there's no way to interactively play the environment as an user. This is helpful for both visualisation and debugging, to make sure your environment is working as intended.\n",
    "\n",
    "Try implementing \"ANSI\" rendering (console text) and write some code that allows you to interactively provide input (in terms of action) at each step. For how rendering works, you may find it useful to refer to the Monty Hall environment at `src/environments/monty_hall` and consult the Gymnasium documentation at the following URLs:\n",
    "- https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\n",
    "- https://gymnasium.farama.org/api/env/#gymnasium.Env.render\n",
    "\n",
    "## Extension 2. Action Masking\n",
    "1) Explore the code for the Monty Hall environment provided in the repository at `src/environments/monty_hall`.\n",
    "2) Explain how Action Masking is implemented there and used in Tabular Q Learning at `src/rl/tabular_q_learning.py`.\n",
    "\n",
    "## Extension 3. Recording your Agent\n",
    "In Notebook 3, we learnt that during training, we can visualise the cumulative returns of our agent to monitor how well they are performing, since a higher reward equates to better performance.\n",
    "\n",
    "Another common method, and a nice visualisation tool, is viewing recordings of how your agent is performing. This requires sometimes additional processing, but you do not have to record your environment at every time step. \n",
    "\n",
    "For this extension, copy your code from Notebook 3 on the Monty Hall environment, instead of using this particular environment (as Tabular Q-Learning is not applicable here). You can proceed by reading [this documentation](https://gymnasium.farama.org/introduction/record_agent/) on the Gymnasium website.\n",
    "\n",
    "## Extension 4. Make the Environment Combinatorial\n",
    "Try adding an additional product to the environment, thus making this environment combinatorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba918f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "With this notebook, we have discovered on how you can model any combinatorial problem as a Reinforcement Learning environment using the Gymnasium API. The Gymnasium API is the most popular API for modelling RL environments, aside from multi-agent RL, which typically uses **PettingZoo** (which is just an extension of Gymnasium to handle both parallel and turn-based multi-agent RL).\n",
    "\n",
    "Notice how the environment we implemented here uses a `Box()` space, so is not directly trainable with our Tabular Q-Learning implementation, but function approximation based extensions of Tabular Q-Learning will allow compatibility. If you are able to implement the visualisation extension, that is more than sufficient. You can implement a stronger algorithm to support this environment (which also serves as your motivation!) after going through the conclusion notebook.\n",
    "\n",
    "The next notebook will conclude this exemplar, however, the conclusion provides a quite comprehensive yet accessible literature review of the state-of-the-art for solving combinatorial problems using RL. Therefore, it is definitely worth reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
